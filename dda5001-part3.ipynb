{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784d3119",
   "metadata": {
    "papermill": {
     "duration": 0.008343,
     "end_time": "2025-12-02T14:03:17.211711",
     "exception": false,
     "start_time": "2025-12-02T14:03:17.203368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part III: Test-time Scaling on Qwen Model\n",
    "\n",
    "## Task 1: Run generation with different temperatures\n",
    "\n",
    "This notebook implements test-time scaling experiments on three math problem-solving benchmarks:\n",
    "- Math500 (500 problems, 16 rollouts per problem)\n",
    "- AMC23 (40 problems, 64 rollouts per problem)\n",
    "- AIME25 (30 problems, 64 rollouts per problem)\n",
    "\n",
    "We evaluate two models with three temperature settings (0.6, 1.0, 1.2) while keeping top-p fixed at 0.9.\n",
    "\n",
    "**Models:**\n",
    "- Base model: Qwen/Qwen2.5-Math-1.5B\n",
    "- GRPO-tuned model: Qwen/Qwen2.5-Math-1.5B-Instruct\n",
    "\n",
    "**Expected outputs:**\n",
    "- For each configuration, we generate JSONL files with model predictions\n",
    "- We then evaluate using pass@k metrics and majority vote aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "628fee1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T14:03:17.227103Z",
     "iopub.status.busy": "2025-12-02T14:03:17.226867Z",
     "iopub.status.idle": "2025-12-02T14:05:03.258463Z",
     "shell.execute_reply": "2025-12-02T14:05:03.257422Z"
    },
    "papermill": {
     "duration": 106.040947,
     "end_time": "2025-12-02T14:05:03.260211",
     "exception": false,
     "start_time": "2025-12-02T14:03:17.219264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\r\n",
      "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\r\n",
      "cudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\r\n",
      "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\r\n",
      "google-adk 1.18.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.26.0 which is incompatible.\r\n",
      "google-adk 1.18.0 requires opentelemetry-exporter-otlp-proto-http>=1.36.0, but you have opentelemetry-exporter-otlp-proto-http 1.26.0 which is incompatible.\r\n",
      "google-adk 1.18.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.26.0 which is incompatible.\r\n",
      "ydata-profiling 4.17.0 requires numba<=0.61,>=0.56.0, but you have numba 0.61.2 which is incompatible.\r\n",
      "a2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.7 which is incompatible.\r\n",
      "opentelemetry-resourcedetector-gcp 1.11.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\r\n",
      "opentelemetry-resourcedetector-gcp 1.11.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\r\n",
      "opentelemetry-exporter-gcp-monitoring 1.11.0a0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\r\n",
      "opentelemetry-exporter-gcp-monitoring 1.11.0a0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\r\n",
      "opentelemetry-exporter-gcp-trace 1.11.0 requires opentelemetry-api~=1.30, but you have opentelemetry-api 1.26.0 which is incompatible.\r\n",
      "opentelemetry-exporter-gcp-trace 1.11.0 requires opentelemetry-sdk~=1.30, but you have opentelemetry-sdk 1.26.0 which is incompatible.\r\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-api>=1.35.0, but you have opentelemetry-api 1.26.0 which is incompatible.\r\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.26.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\r\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\r\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\r\n",
      "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\r\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\r\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.9/207.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n"
     ]
    }
   ],
   "source": [
    "# this notebook is for kaggle\n",
    "!pip install --no-index --find-links=/kaggle/input/it-is-vllm-0-8-5 -q vllm\n",
    "!pip install -q pylatexenc math-verify[antlr4_9_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4d174c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T14:05:03.278644Z",
     "iopub.status.busy": "2025-12-02T14:05:03.277796Z",
     "iopub.status.idle": "2025-12-02T14:05:03.663508Z",
     "shell.execute_reply": "2025-12-02T14:05:03.662605Z"
    },
    "papermill": {
     "duration": 0.396756,
     "end_time": "2025-12-02T14:05:03.664961",
     "exception": false,
     "start_time": "2025-12-02T14:05:03.268205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/src\n",
      "evaluate.py  inference.py  verifier\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir /kaggle/working/src\n",
    "!cp -r /kaggle/input/mlproject/* /kaggle/working/src\n",
    "\n",
    "%cd /kaggle/working/src\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642fdcf6",
   "metadata": {
    "papermill": {
     "duration": 0.007302,
     "end_time": "2025-12-02T14:05:03.679823",
     "exception": false,
     "start_time": "2025-12-02T14:05:03.672521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## I. Base Model: Qwen/Qwen2.5-Math-1.5B\n",
    "\n",
    "### Math500 Dataset (rollout-n = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7a86d1",
   "metadata": {
    "papermill": {
     "duration": 0.007012,
     "end_time": "2025-12-02T14:05:03.694135",
     "exception": false,
     "start_time": "2025-12-02T14:05:03.687123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Temperature = 0.6 (more deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6333d942",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T14:05:03.709833Z",
     "iopub.status.busy": "2025-12-02T14:05:03.709592Z",
     "iopub.status.idle": "2025-12-02T15:07:28.121095Z",
     "shell.execute_reply": "2025-12-02T15:07:28.120357Z"
    },
    "papermill": {
     "duration": 3744.421312,
     "end_time": "2025-12-02T15:07:28.122690",
     "exception": false,
     "start_time": "2025-12-02T14:05:03.701378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 14:05:16.329676: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764684316.509205      61 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764684316.567206      61 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 14:05:34 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\r\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\r\n",
      "2025-12-02 14:05:43.682068: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-12-02 14:05:43.682125: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764684343.702103      80 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764684343.702281      79 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764684343.709096      79 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "E0000 00:00:1764684343.709256      80 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 14:05:50 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "INFO 12-02 14:05:50 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "[Rank 0] Logical GPU count: 1\r\n",
      "[Rank 0] Loading dataset: math -> math-ai/math500\r\n",
      "[Rank 1] Logical GPU count: 1\r\n",
      "[Rank 1] Loading dataset: math -> math-ai/math500\r\n",
      "README.md: 100%|███████████████████████████████| 412/412 [00:00<00:00, 3.21MB/s]\r\n",
      "test.jsonl: 447kB [00:00, 20.2MB/s]\r\n",
      "Generating test split: 100%|████████| 500/500 [00:00<00:00, 18764.28 examples/s]\r\n",
      "tokenizer_config.json: 7.32kB [00:00, 23.8MB/s]\r\n",
      "vocab.json: 2.78MB [00:00, 67.5MB/s]\r\n",
      "merges.txt: 1.67MB [00:00, 140MB/s]\r\n",
      "tokenizer.json: 7.03MB [00:00, 175MB/s]\r\n",
      "[Rank 1] Needs to process 250 prompts (indices [250, 500))\r\n",
      "[Rank 0] Needs to process 250 prompts (indices [0, 250))\r\n",
      "config.json: 100%|█████████████████████████████| 676/676 [00:00<00:00, 5.91MB/s]\r\n",
      "WARNING 12-02 14:06:01 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 12-02 14:06:01 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-02 14:06:26 [config.py:717] This model supports multiple tasks: {'reward', 'generate', 'embed', 'score', 'classify'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 14:06:26 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 14:06:26 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 14:06:26 [config.py:717] This model supports multiple tasks: {'classify', 'embed', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 14:06:26 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 14:06:26 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "generation_config.json: 100%|██████████████████| 138/138 [00:00<00:00, 1.14MB/s]\r\n",
      "INFO 12-02 14:06:28 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 14:06:28 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 12-02 14:06:28 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 14:06:28 [cuda.py:289] Using XFormers backend.\r\n",
      "[W1202 14:06:29.550930116 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 14:06:29.551729344 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 14:06:29.551978779 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 14:06:29.552573594 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 14:06:29 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 14:06:29 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 14:06:29 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B...\r\n",
      "INFO 12-02 14:06:29 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B...\r\n",
      "INFO 12-02 14:06:30 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 14:06:30 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "model.safetensors: 100%|████████████████████| 3.09G/3.09G [00:08<00:00, 382MB/s]\r\n",
      "INFO 12-02 14:06:38 [weight_utils.py:281] Time spent downloading weights for Qwen/Qwen2.5-Math-1.5B: 8.327670 seconds\r\n",
      "INFO 12-02 14:06:38 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "INFO 12-02 14:06:38 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.55s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.55s/it]\r\n",
      "\r\n",
      "INFO 12-02 14:06:42 [loader.py:458] Loading weights took 3.63 seconds\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.55s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.55s/it]\r\n",
      "\r\n",
      "INFO 12-02 14:06:42 [loader.py:458] Loading weights took 3.75 seconds\r\n",
      "INFO 12-02 14:06:42 [model_runner.py:1140] Model loading took 2.8798 GiB and 12.429414 seconds\r\n",
      "INFO 12-02 14:06:42 [model_runner.py:1140] Model loading took 2.8798 GiB and 12.779482 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 14:06:44 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 14:06:44 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 14:06:44 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 14:06:44 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 14:06:44 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 14:06:44 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 14:06:50 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 12-02 14:06:50 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.04it/s]\r\n",
      "INFO 12-02 14:07:23 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.19 GiB\r\n",
      "INFO 12-02 14:07:23 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 41.42 seconds\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.05it/s]\r\n",
      "INFO 12-02 14:07:23 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\r\n",
      "INFO 12-02 14:07:23 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 41.10 seconds\r\n",
      "[Rank 0] Starting generation for 250 prompts with batch_size=16\r\n",
      "[Rank 1] Starting generation for 250 prompts with batch_size=16\r\n",
      "Processed prompts: 100%|█| 256/256 [03:19<00:00,  1.28it/s, est. speed input: 15\r\n",
      "[Rank 0] Processed 16/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [04:36<00:00,  1.08s/it, est. speed input: 78\r\n",
      "[Rank 1] Processed 16/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:27<00:00,  1.24it/s, est. speed input: 11\r\n",
      "[Rank 0] Processed 32/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:36<00:00,  1.18it/s, est. speed input: 12\r\n",
      "[Rank 1] Processed 32/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:39<00:00,  1.16it/s, est. speed input: 89\r\n",
      "[Rank 0] Processed 48/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:05<00:00,  1.38it/s, est. speed input: 15\r\n",
      "[Rank 1] Processed 48/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:38<00:00,  1.17it/s, est. speed input: 87\r\n",
      "[Rank 0] Processed 64/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:30<00:00,  1.21it/s, est. speed input: 15\r\n",
      "[Rank 1] Processed 64/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:09<00:00,  1.35it/s, est. speed input: 13\r\n",
      "[Rank 1] Processed 80/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [04:11<00:00,  1.02it/s, est. speed input: 70\r\n",
      "[Rank 0] Processed 80/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:13<00:00,  1.32it/s, est. speed input: 12\r\n",
      "[Rank 0] Processed 96/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [04:03<00:00,  1.05it/s, est. speed input: 76\r\n",
      "[Rank 1] Processed 96/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:41<00:00,  1.16it/s, est. speed input: 11\r\n",
      "[Rank 1] Processed 112/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [04:46<00:00,  1.12s/it, est. speed input: 11\r\n",
      "[Rank 0] Processed 112/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:22<00:00,  1.26it/s, est. speed input: 12\r\n",
      "[Rank 1] Processed 128/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:58<00:00,  1.07it/s, est. speed input: 94\r\n",
      "[Rank 0] Processed 128/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:55<00:00,  1.09it/s, est. speed input: 12\r\n",
      "[Rank 1] Processed 144/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:51<00:00,  1.11it/s, est. speed input: 93\r\n",
      "[Rank 0] Processed 144/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [04:22<00:00,  1.02s/it, est. speed input: 88\r\n",
      "[Rank 1] Processed 160/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:44<00:00,  1.14it/s, est. speed input: 12\r\n",
      "[Rank 0] Processed 160/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [04:00<00:00,  1.06it/s, est. speed input: 12\r\n",
      "[Rank 0] Processed 176/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [04:35<00:00,  1.08s/it, est. speed input: 79\r\n",
      "[Rank 1] Processed 176/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:37<00:00,  1.18it/s, est. speed input: 90\r\n",
      "[Rank 1] Processed 192/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [04:27<00:00,  1.04s/it, est. speed input: 11\r\n",
      "[Rank 0] Processed 192/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:44<00:00,  1.14it/s, est. speed input: 10\r\n",
      "[Rank 1] Processed 208/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:36<00:00,  1.18it/s, est. speed input: 10\r\n",
      "[Rank 0] Processed 208/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:54<00:00,  1.46it/s, est. speed input: 16\r\n",
      "[Rank 1] Processed 224/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:26<00:00,  1.24it/s, est. speed input: 16\r\n",
      "[Rank 0] Processed 224/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:44<00:00,  1.14it/s, est. speed input: 11\r\n",
      "[Rank 1] Processed 240/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [04:02<00:00,  1.05it/s, est. speed input: 98\r\n",
      "[Rank 0] Processed 240/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 160/160 [02:27<00:00,  1.08it/s, est. speed input: 10\r\n",
      "[Rank 1] Processed 250/250 prompts with n=16 completions each\r\n",
      "[Rank 1] Saved partial JSONL to outputs/math500_base_t0.6.rank1.jsonl\r\n",
      "[rank0]:[W1202 15:05:56.235216351 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Processed prompts: 100%|█| 160/160 [02:28<00:00,  1.08it/s, est. speed input: 10\r\n",
      "[Rank 0] Processed 250/250 prompts with n=16 completions each\r\n",
      "[Rank 0] Saved partial JSONL to outputs/math500_base_t0.6.rank0.jsonl\r\n",
      "[rank0]:[W1202 15:07:23.147835118 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Total time consuming is: 3706.10s\r\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/math500_base_t0.6.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B\" \\\n",
    "  --dataset \"math\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 16 \\\n",
    "  --temperature 0.6 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/math500_base_t0.6.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a36a523",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T15:07:28.158433Z",
     "iopub.status.busy": "2025-12-02T15:07:28.158180Z",
     "iopub.status.idle": "2025-12-02T15:08:00.687310Z",
     "shell.execute_reply": "2025-12-02T15:08:00.686508Z"
    },
    "papermill": {
     "duration": 32.548845,
     "end_time": "2025-12-02T15:08:00.689278",
     "exception": false,
     "start_time": "2025-12-02T15:07:28.140433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/math500_base_t0.6.jsonl...\r\n",
      "Scoring generations from outputs/math500_base_t0.6.jsonl...\r\n",
      "Processing lines: 100%|████████████████████| 8000/8000 [00:31<00:00, 256.46it/s]\r\n",
      "Processing complete. Scored 8000 lines across 500 unique problems.\r\n",
      "\r\n",
      "Pass@k Metrics:\r\n",
      "  pass@1   : 35.99%\r\n",
      "  pass@2   : 51.28%\r\n",
      "  pass@4   : 65.74%\r\n",
      "  pass@8   : 77.06%\r\n",
      "  pass@16  : 84.60%\r\n",
      "\r\n",
      "Majority Vote Metric:\r\n",
      "  maj@1    : 45.60%\r\n",
      "\r\n",
      "Scored results saved to outputs/math500_base_t0.6_eval.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py \\\n",
    "  --input_file outputs/math500_base_t0.6.jsonl \\\n",
    "  --output_file outputs/math500_base_t0.6_eval.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a8db09",
   "metadata": {
    "papermill": {
     "duration": 0.024129,
     "end_time": "2025-12-02T15:08:00.738446",
     "exception": false,
     "start_time": "2025-12-02T15:08:00.714317",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Temperature = 1.0 (original distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3dc6e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T15:08:00.790200Z",
     "iopub.status.busy": "2025-12-02T15:08:00.789511Z",
     "iopub.status.idle": "2025-12-02T16:05:51.678282Z",
     "shell.execute_reply": "2025-12-02T16:05:51.677516Z"
    },
    "papermill": {
     "duration": 3470.916838,
     "end_time": "2025-12-02T16:05:51.680002",
     "exception": false,
     "start_time": "2025-12-02T15:08:00.763164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 15:08:05.122668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764688085.147077     266 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764688085.155083     266 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 15:08:11 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\r\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\r\n",
      "2025-12-02 15:08:19.248150: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764688099.269465     284 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764688099.275448     284 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-12-02 15:08:19.277885: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764688099.299629     285 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764688099.305624     285 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 15:08:25 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "INFO 12-02 15:08:25 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "[Rank 0] Logical GPU count: 1\r\n",
      "[Rank 0] Loading dataset: math -> math-ai/math500\r\n",
      "[Rank 1] Logical GPU count: 1\r\n",
      "[Rank 1] Loading dataset: math -> math-ai/math500\r\n",
      "[Rank 0] Needs to process 250 prompts (indices [0, 250))\r\n",
      "WARNING 12-02 15:08:35 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "[Rank 1] Needs to process 250 prompts (indices [250, 500))\r\n",
      "WARNING 12-02 15:08:36 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-02 15:08:59 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 15:08:59 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 15:08:59 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 15:09:00 [config.py:717] This model supports multiple tasks: {'reward', 'generate', 'score', 'embed', 'classify'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 15:09:00 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 15:09:00 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 15:09:01 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 15:09:01 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 12-02 15:09:02 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 15:09:02 [cuda.py:289] Using XFormers backend.\r\n",
      "[W1202 15:09:02.394382309 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 15:09:02.395138114 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 15:09:02 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 15:09:02 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B...\r\n",
      "INFO 12-02 15:09:02 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 15:09:02 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "[W1202 15:09:02.889620394 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 15:09:02.890327310 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 15:09:03 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 15:09:03 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B...\r\n",
      "INFO 12-02 15:09:03 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 15:09:03 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.37s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.37s/it]\r\n",
      "\r\n",
      "INFO 12-02 15:09:06 [loader.py:458] Loading weights took 3.46 seconds\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.43s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.44s/it]\r\n",
      "\r\n",
      "INFO 12-02 15:09:06 [loader.py:458] Loading weights took 3.52 seconds\r\n",
      "INFO 12-02 15:09:07 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.898468 seconds\r\n",
      "INFO 12-02 15:09:07 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.970976 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 15:09:08 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 15:09:09 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 15:09:09 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 15:09:09 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 15:09:09 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 15:09:09 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 15:09:13 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 12-02 15:09:13 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.05it/s]\r\n",
      "INFO 12-02 15:09:46 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\r\n",
      "INFO 12-02 15:09:46 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.66 seconds\r\n",
      "[Rank 0] Starting generation for 250 prompts with batch_size=16\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.05it/s]\r\n",
      "INFO 12-02 15:09:47 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\r\n",
      "INFO 12-02 15:09:47 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.73 seconds\r\n",
      "[Rank 1] Starting generation for 250 prompts with batch_size=16\r\n",
      "Processed prompts: 100%|█| 256/256 [03:22<00:00,  1.26it/s, est. speed input: 10\r\n",
      "[Rank 1] Processed 16/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:38<00:00,  1.17it/s, est. speed input: 14\r\n",
      "[Rank 0] Processed 16/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:15<00:00,  1.31it/s, est. speed input: 13\r\n",
      "[Rank 1] Processed 32/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:19<00:00,  1.28it/s, est. speed input: 12\r\n",
      "[Rank 0] Processed 32/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:10<00:00,  1.34it/s, est. speed input: 15\r\n",
      "[Rank 1] Processed 48/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:15<00:00,  1.31it/s, est. speed input: 10\r\n",
      "[Rank 0] Processed 48/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:12<00:00,  1.33it/s, est. speed input: 17\r\n",
      "[Rank 1] Processed 64/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:17<00:00,  1.30it/s, est. speed input: 96\r\n",
      "[Rank 0] Processed 64/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:57<00:00,  1.44it/s, est. speed input: 13\r\n",
      "[Rank 1] Processed 80/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:33<00:00,  1.20it/s, est. speed input: 83\r\n",
      "[Rank 0] Processed 80/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:18<00:00,  1.29it/s, est. speed input: 93\r\n",
      "[Rank 1] Processed 96/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:19<00:00,  1.28it/s, est. speed input: 12\r\n",
      "[Rank 0] Processed 96/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:27<00:00,  1.24it/s, est. speed input: 11\r\n",
      "[Rank 1] Processed 112/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [04:13<00:00,  1.01it/s, est. speed input: 13\r\n",
      "[Rank 0] Processed 112/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:22<00:00,  1.26it/s, est. speed input: 12\r\n",
      "[Rank 1] Processed 128/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:25<00:00,  1.25it/s, est. speed input: 10\r\n",
      "[Rank 0] Processed 128/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:19<00:00,  1.29it/s, est. speed input: 14\r\n",
      "[Rank 1] Processed 144/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:19<00:00,  1.29it/s, est. speed input: 10\r\n",
      "[Rank 0] Processed 144/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:40<00:00,  1.16it/s, est. speed input: 10\r\n",
      "[Rank 1] Processed 160/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:43<00:00,  1.14it/s, est. speed input: 12\r\n",
      "[Rank 0] Processed 160/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:40<00:00,  1.16it/s, est. speed input: 99\r\n",
      "[Rank 1] Processed 176/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:44<00:00,  1.14it/s, est. speed input: 13\r\n",
      "[Rank 0] Processed 176/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:22<00:00,  1.27it/s, est. speed input: 97\r\n",
      "[Rank 1] Processed 192/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:44<00:00,  1.14it/s, est. speed input: 13\r\n",
      "[Rank 0] Processed 192/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:47<00:00,  1.13it/s, est. speed input: 10\r\n",
      "[Rank 1] Processed 208/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:29<00:00,  1.22it/s, est. speed input: 10\r\n",
      "[Rank 0] Processed 208/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:58<00:00,  1.43it/s, est. speed input: 15\r\n",
      "[Rank 1] Processed 224/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:37<00:00,  1.18it/s, est. speed input: 15\r\n",
      "[Rank 0] Processed 224/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:45<00:00,  1.14it/s, est. speed input: 11\r\n",
      "[Rank 1] Processed 240/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 160/160 [02:09<00:00,  1.24it/s, est. speed input: 12\r\n",
      "[Rank 1] Processed 250/250 prompts with n=16 completions each\r\n",
      "[Rank 1] Saved partial JSONL to outputs/math500_base_t1.0.rank1.jsonl\r\n",
      "[rank0]:[W1202 16:02:42.287040119 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Processed prompts: 100%|█| 256/256 [03:38<00:00,  1.17it/s, est. speed input: 10\r\n",
      "[Rank 0] Processed 240/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 160/160 [02:35<00:00,  1.03it/s, est. speed input: 10\r\n",
      "[Rank 0] Processed 250/250 prompts with n=16 completions each\r\n",
      "[Rank 0] Saved partial JSONL to outputs/math500_base_t1.0.rank0.jsonl\r\n",
      "[rank0]:[W1202 16:05:46.763185370 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Total time consuming is: 3454.15s\r\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/math500_base_t1.0.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B\" \\\n",
    "  --dataset \"math\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 16 \\\n",
    "  --temperature 1.0 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/math500_base_t1.0.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a43dbfd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T16:05:51.749658Z",
     "iopub.status.busy": "2025-12-02T16:05:51.748952Z",
     "iopub.status.idle": "2025-12-02T16:06:39.957978Z",
     "shell.execute_reply": "2025-12-02T16:06:39.957210Z"
    },
    "papermill": {
     "duration": 48.246892,
     "end_time": "2025-12-02T16:06:39.959579",
     "exception": false,
     "start_time": "2025-12-02T16:05:51.712687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/math500_base_t1.0.jsonl...\r\n",
      "Scoring generations from outputs/math500_base_t1.0.jsonl...\r\n",
      "Processing lines:  33%|██████▌             | 2619/8000 [00:15<00:28, 190.70it/s]Timeout during comparison\r\n",
      "Processing lines:  52%|██████████▍         | 4184/8000 [00:23<00:14, 272.15it/s]Timeout during comparison\r\n",
      "Processing lines: 100%|████████████████████| 8000/8000 [00:46<00:00, 170.98it/s]\r\n",
      "Processing complete. Scored 8000 lines across 500 unique problems.\r\n",
      "\r\n",
      "Pass@k Metrics:\r\n",
      "  pass@1   : 28.85%\r\n",
      "  pass@2   : 44.83%\r\n",
      "  pass@4   : 61.93%\r\n",
      "  pass@8   : 76.61%\r\n",
      "  pass@16  : 86.00%\r\n",
      "\r\n",
      "Majority Vote Metric:\r\n",
      "  maj@1    : 41.40%\r\n",
      "\r\n",
      "Scored results saved to outputs/math500_base_t1.0_eval.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py \\\n",
    "  --input_file outputs/math500_base_t1.0.jsonl \\\n",
    "  --output_file outputs/math500_base_t1.0_eval.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca917bb",
   "metadata": {
    "papermill": {
     "duration": 0.042171,
     "end_time": "2025-12-02T16:06:40.046006",
     "exception": false,
     "start_time": "2025-12-02T16:06:40.003835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Temperature = 1.2 (more random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1272458",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T16:06:40.131571Z",
     "iopub.status.busy": "2025-12-02T16:06:40.131000Z",
     "iopub.status.idle": "2025-12-02T17:02:00.881258Z",
     "shell.execute_reply": "2025-12-02T17:02:00.880508Z"
    },
    "papermill": {
     "duration": 3320.794565,
     "end_time": "2025-12-02T17:02:00.882758",
     "exception": false,
     "start_time": "2025-12-02T16:06:40.088193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 16:06:44.107478: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764691604.128499     437 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764691604.134815     437 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 16:06:50 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\r\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\r\n",
      "2025-12-02 16:06:58.159315: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-12-02 16:06:58.170439: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764691618.179928     455 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764691618.186208     455 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764691618.191638     456 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764691618.197913     456 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 16:07:04 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "INFO 12-02 16:07:04 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "[Rank 0] Logical GPU count: 1\r\n",
      "[Rank 0] Loading dataset: math -> math-ai/math500\r\n",
      "[Rank 1] Logical GPU count: 1\r\n",
      "[Rank 1] Loading dataset: math -> math-ai/math500\r\n",
      "[Rank 1] Needs to process 250 prompts (indices [250, 500))\r\n",
      "[Rank 0] Needs to process 250 prompts (indices [0, 250))\r\n",
      "WARNING 12-02 16:07:14 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 12-02 16:07:14 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-02 16:07:39 [config.py:717] This model supports multiple tasks: {'generate', 'classify', 'score', 'reward', 'embed'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 16:07:39 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 16:07:39 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 16:07:39 [config.py:717] This model supports multiple tasks: {'classify', 'reward', 'generate', 'score', 'embed'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 16:07:39 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 16:07:39 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 16:07:41 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 16:07:41 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 12-02 16:07:41 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 16:07:41 [cuda.py:289] Using XFormers backend.\r\n",
      "[W1202 16:07:41.900849685 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 16:07:41.901543510 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 16:07:42 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 16:07:42 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B...\r\n",
      "[W1202 16:07:42.040706893 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 16:07:42.041472537 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 16:07:42 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 16:07:42 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B...\r\n",
      "INFO 12-02 16:07:42 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 16:07:42 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 16:07:42 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "INFO 12-02 16:07:42 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.48s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.48s/it]\r\n",
      "\r\n",
      "INFO 12-02 16:07:46 [loader.py:458] Loading weights took 3.57 seconds\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.49s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.49s/it]\r\n",
      "\r\n",
      "INFO 12-02 16:07:46 [loader.py:458] Loading weights took 3.58 seconds\r\n",
      "INFO 12-02 16:07:46 [model_runner.py:1140] Model loading took 2.8798 GiB and 4.246835 seconds\r\n",
      "INFO 12-02 16:07:46 [model_runner.py:1140] Model loading took 2.8798 GiB and 4.248587 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 16:07:48 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 16:07:48 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 16:07:48 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 16:07:48 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 16:07:48 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 16:07:48 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 16:07:53 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 12-02 16:07:53 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.04it/s]\r\n",
      "INFO 12-02 16:08:26 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.19 GiB\r\n",
      "INFO 12-02 16:08:26 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.86 seconds\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.04it/s]\r\n",
      "INFO 12-02 16:08:26 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.19 GiB\r\n",
      "INFO 12-02 16:08:26 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 40.05 seconds\r\n",
      "[Rank 0] Starting generation for 250 prompts with batch_size=16\r\n",
      "Processed prompts:   0%| | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s,[Rank 1] Starting generation for 250 prompts with batch_size=16\r\n",
      "Processed prompts: 100%|█| 256/256 [03:12<00:00,  1.33it/s, est. speed input: 11\r\n",
      "[Rank 1] Processed 16/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:34<00:00,  1.19it/s, est. speed input: 14\r\n",
      "[Rank 0] Processed 16/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:14<00:00,  1.32it/s, est. speed input: 13\r\n",
      "[Rank 1] Processed 32/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:24<00:00,  1.25it/s, est. speed input: 12\r\n",
      "[Rank 0] Processed 32/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:18<00:00,  1.29it/s, est. speed input: 14\r\n",
      "[Rank 1] Processed 48/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:09<00:00,  1.35it/s, est. speed input: 10\r\n",
      "[Rank 0] Processed 48/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:17<00:00,  1.30it/s, est. speed input: 17\r\n",
      "[Rank 1] Processed 64/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:03<00:00,  1.40it/s, est. speed input: 10\r\n",
      "[Rank 0] Processed 64/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:30<00:00,  1.22it/s, est. speed input: 11\r\n",
      "[Rank 1] Processed 80/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:22<00:00,  1.26it/s, est. speed input: 87\r\n",
      "[Rank 0] Processed 80/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:06<00:00,  1.37it/s, est. speed input: 99\r\n",
      "[Rank 1] Processed 96/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:12<00:00,  1.33it/s, est. speed input: 12\r\n",
      "[Rank 0] Processed 96/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:22<00:00,  1.27it/s, est. speed input: 12\r\n",
      "[Rank 1] Processed 112/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [04:02<00:00,  1.05it/s, est. speed input: 13\r\n",
      "[Rank 0] Processed 112/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:18<00:00,  1.29it/s, est. speed input: 12\r\n",
      "[Rank 1] Processed 128/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:35<00:00,  1.19it/s, est. speed input: 10\r\n",
      "[Rank 0] Processed 128/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:12<00:00,  1.33it/s, est. speed input: 15\r\n",
      "[Rank 1] Processed 144/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:10<00:00,  1.35it/s, est. speed input: 11\r\n",
      "[Rank 0] Processed 144/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:25<00:00,  1.24it/s, est. speed input: 11\r\n",
      "[Rank 1] Processed 160/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:25<00:00,  1.25it/s, est. speed input: 13\r\n",
      "[Rank 0] Processed 160/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:29<00:00,  1.22it/s, est. speed input: 10\r\n",
      "[Rank 1] Processed 176/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:25<00:00,  1.25it/s, est. speed input: 15\r\n",
      "[Rank 0] Processed 176/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:21<00:00,  1.27it/s, est. speed input: 97\r\n",
      "[Rank 1] Processed 192/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:33<00:00,  1.20it/s, est. speed input: 14\r\n",
      "[Rank 0] Processed 192/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:07<00:00,  1.36it/s, est. speed input: 12\r\n",
      "[Rank 1] Processed 208/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:24<00:00,  1.25it/s, est. speed input: 10\r\n",
      "[Rank 0] Processed 208/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:01<00:00,  1.41it/s, est. speed input: 15\r\n",
      "[Rank 1] Processed 224/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:16<00:00,  1.30it/s, est. speed input: 16\r\n",
      "[Rank 0] Processed 224/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:37<00:00,  1.18it/s, est. speed input: 11\r\n",
      "[Rank 1] Processed 240/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:29<00:00,  1.22it/s, est. speed input: 11\r\n",
      "[Rank 0] Processed 240/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 160/160 [02:26<00:00,  1.09it/s, est. speed input: 10\r\n",
      "[Rank 1] Processed 250/250 prompts with n=16 completions each\r\n",
      "[Rank 1] Saved partial JSONL to outputs/math500_base_t1.2.rank1.jsonl\r\n",
      "[rank0]:[W1202 17:00:33.457470326 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Processed prompts: 100%|█| 160/160 [02:14<00:00,  1.19it/s, est. speed input: 11\r\n",
      "[Rank 0] Processed 250/250 prompts with n=16 completions each\r\n",
      "[Rank 0] Saved partial JSONL to outputs/math500_base_t1.2.rank0.jsonl\r\n",
      "[rank0]:[W1202 17:01:55.850705482 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Total time consuming is: 3304.35s\r\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/math500_base_t1.2.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B\" \\\n",
    "  --dataset \"math\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 16 \\\n",
    "  --temperature 1.2 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/math500_base_t1.2.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b4f6bd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:02:00.986139Z",
     "iopub.status.busy": "2025-12-02T17:02:00.985872Z",
     "iopub.status.idle": "2025-12-02T17:02:45.037940Z",
     "shell.execute_reply": "2025-12-02T17:02:45.037131Z"
    },
    "papermill": {
     "duration": 44.106399,
     "end_time": "2025-12-02T17:02:45.039941",
     "exception": false,
     "start_time": "2025-12-02T17:02:00.933542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/math500_base_t1.2.jsonl...\r\n",
      "Scoring generations from outputs/math500_base_t1.2.jsonl...\r\n",
      "Processing lines:   8%|█▋                   | 629/8000 [00:03<00:31, 233.63it/s]Timeout during comparison\r\n",
      "Processing lines:  50%|██████████          | 4029/8000 [00:20<00:31, 124.60it/s]Timeout during comparison\r\n",
      "Processing lines:  91%|██████████████████▏ | 7266/8000 [00:36<00:02, 305.53it/s]Timeout during comparison\r\n",
      "Processing lines: 100%|████████████████████| 8000/8000 [00:42<00:00, 187.97it/s]\r\n",
      "Processing complete. Scored 8000 lines across 500 unique problems.\r\n",
      "\r\n",
      "Pass@k Metrics:\r\n",
      "  pass@1   : 18.80%\r\n",
      "  pass@2   : 31.79%\r\n",
      "  pass@4   : 48.68%\r\n",
      "  pass@8   : 66.21%\r\n",
      "  pass@16  : 80.00%\r\n",
      "\r\n",
      "Majority Vote Metric:\r\n",
      "  maj@1    : 17.00%\r\n",
      "\r\n",
      "Scored results saved to outputs/math500_base_t1.2_eval.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py \\\n",
    "  --input_file outputs/math500_base_t1.2.jsonl \\\n",
    "  --output_file outputs/math500_base_t1.2_eval.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edf1824",
   "metadata": {
    "papermill": {
     "duration": 0.057833,
     "end_time": "2025-12-02T17:02:45.157541",
     "exception": false,
     "start_time": "2025-12-02T17:02:45.099708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### AMC23 Dataset (rollout-n = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243284f5",
   "metadata": {
    "papermill": {
     "duration": 0.057712,
     "end_time": "2025-12-02T17:02:45.273001",
     "exception": false,
     "start_time": "2025-12-02T17:02:45.215289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Temperature = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b95151d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:02:45.391041Z",
     "iopub.status.busy": "2025-12-02T17:02:45.390424Z",
     "iopub.status.idle": "2025-12-02T17:25:14.819411Z",
     "shell.execute_reply": "2025-12-02T17:25:14.818649Z"
    },
    "papermill": {
     "duration": 1349.490125,
     "end_time": "2025-12-02T17:25:14.820830",
     "exception": false,
     "start_time": "2025-12-02T17:02:45.330705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:02:49.303038: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764694969.326800     608 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764694969.334047     608 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 17:02:55 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\r\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\r\n",
      "2025-12-02 17:03:03.087007: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-12-02 17:03:03.087479: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764694983.107281     626 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764694983.108109     627 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764694983.113606     626 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "E0000 00:00:1764694983.114571     627 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 17:03:09 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "INFO 12-02 17:03:09 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "[Rank 0] Logical GPU count: 1\r\n",
      "[Rank 0] Loading dataset: amc -> math-ai/amc23\r\n",
      "[Rank 1] Logical GPU count: 1\r\n",
      "[Rank 1] Loading dataset: amc -> math-ai/amc23\r\n",
      "README.md: 100%|███████████████████████████████| 374/374 [00:00<00:00, 3.93MB/s]\r\n",
      "test-00000-of-00001.parquet: 100%|█████████| 11.9k/11.9k [00:00<00:00, 39.4kB/s]\r\n",
      "Generating test split: 100%|████████████| 40/40 [00:00<00:00, 933.26 examples/s]\r\n",
      "[Rank 0] Needs to process 20 prompts (indices [0, 20))\r\n",
      "[Rank 1] Needs to process 20 prompts (indices [20, 40))\r\n",
      "WARNING 12-02 17:03:20 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 12-02 17:03:20 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-02 17:03:44 [config.py:717] This model supports multiple tasks: {'classify', 'score', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 17:03:44 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 17:03:44 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 17:03:44 [config.py:717] This model supports multiple tasks: {'classify', 'generate', 'score', 'embed', 'reward'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 17:03:44 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 17:03:44 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 17:03:46 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 17:03:46 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 12-02 17:03:46 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 17:03:46 [cuda.py:289] Using XFormers backend.\r\n",
      "[W1202 17:03:47.324066304 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 17:03:47.324809353 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 17:03:47 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 17:03:47 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B...\r\n",
      "[W1202 17:03:47.340094335 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 17:03:47.340826822 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 17:03:47 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 17:03:47 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B...\r\n",
      "INFO 12-02 17:03:47 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 17:03:47 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 17:03:47 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "INFO 12-02 17:03:48 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.37s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.37s/it]\r\n",
      "\r\n",
      "INFO 12-02 17:03:51 [loader.py:458] Loading weights took 3.43 seconds\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.35s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.35s/it]\r\n",
      "\r\n",
      "INFO 12-02 17:03:51 [loader.py:458] Loading weights took 3.43 seconds\r\n",
      "INFO 12-02 17:03:51 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.863680 seconds\r\n",
      "INFO 12-02 17:03:51 [model_runner.py:1140] Model loading took 2.8798 GiB and 4.005046 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 17:03:53 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 17:03:53 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 17:03:53 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 17:03:53 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 17:03:53 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 17:03:53 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 17:03:58 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 12-02 17:03:58 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:34<00:00,  1.03it/s]\r\n",
      "INFO 12-02 17:04:32 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.19 GiB\r\n",
      "INFO 12-02 17:04:32 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 40.69 seconds\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:34<00:00,  1.03it/s]\r\n",
      "INFO 12-02 17:04:32 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.19 GiB\r\n",
      "INFO 12-02 17:04:32 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 40.56 seconds\r\n",
      "[Rank 0] Starting generation for 20 prompts with batch_size=16\r\n",
      "[Rank 1] Starting generation for 20 prompts with batch_size=16\r\n",
      "Processed prompts:   0%| | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/sWARNING 12-02 17:09:16 [scheduler.py:1768] Sequence group 6_parallel_sample_13 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\r\n",
      "WARNING 12-02 17:09:47 [scheduler.py:1768] Sequence group 5_parallel_sample_21 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\r\n",
      "Processed prompts: 100%|█| 1024/1024 [12:33<00:00,  1.36it/s, est. speed input: \r\n",
      "[Rank 1] Processed 16/20 prompts with n=64 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:19<00:00,  1.29it/s, est. speed input: 11\r\n",
      "[Rank 1] Processed 20/20 prompts with n=64 completions each\r\n",
      "[Rank 1] Saved partial JSONL to outputs/amc23_base_t0.6.rank1.jsonl\r\n",
      "[rank0]:[W1202 17:20:28.436785607 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Processed prompts: 100%|█| 1024/1024 [16:56<00:00,  1.01it/s, est. speed input: \r\n",
      "[Rank 0] Processed 16/20 prompts with n=64 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:38<00:00,  1.17it/s, est. speed input: 10\r\n",
      "[Rank 0] Processed 20/20 prompts with n=64 completions each\r\n",
      "[Rank 0] Saved partial JSONL to outputs/amc23_base_t0.6.rank0.jsonl\r\n",
      "[rank0]:[W1202 17:25:10.006564134 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Total time consuming is: 1333.46s\r\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/amc23_base_t0.6.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B\" \\\n",
    "  --dataset \"amc\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 64 \\\n",
    "  --temperature 0.6 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/amc23_base_t0.6.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c084079",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:25:14.948928Z",
     "iopub.status.busy": "2025-12-02T17:25:14.948675Z",
     "iopub.status.idle": "2025-12-02T17:25:22.114401Z",
     "shell.execute_reply": "2025-12-02T17:25:22.113331Z"
    },
    "papermill": {
     "duration": 7.231831,
     "end_time": "2025-12-02T17:25:22.116585",
     "exception": false,
     "start_time": "2025-12-02T17:25:14.884754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/amc23_base_t0.6.jsonl...\r\n",
      "Scoring generations from outputs/amc23_base_t0.6.jsonl...\r\n",
      "Processing lines: 100%|████████████████████| 2560/2560 [00:06<00:00, 413.79it/s]\r\n",
      "Processing complete. Scored 2560 lines across 40 unique problems.\r\n",
      "\r\n",
      "Pass@k Metrics:\r\n",
      "  pass@1   : 34.45%\r\n",
      "  pass@2   : 48.57%\r\n",
      "  pass@4   : 61.68%\r\n",
      "  pass@8   : 72.23%\r\n",
      "  pass@16  : 80.21%\r\n",
      "  pass@32  : 86.84%\r\n",
      "  pass@64  : 92.50%\r\n",
      "\r\n",
      "Majority Vote Metric:\r\n",
      "  maj@1    : 50.00%\r\n",
      "\r\n",
      "Scored results saved to outputs/amc23_base_t0.6_eval.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py \\\n",
    "  --input_file outputs/amc23_base_t0.6.jsonl \\\n",
    "  --output_file outputs/amc23_base_t0.6_eval.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4fa259",
   "metadata": {
    "papermill": {
     "duration": 0.066376,
     "end_time": "2025-12-02T17:25:22.250184",
     "exception": false,
     "start_time": "2025-12-02T17:25:22.183808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e1d9207",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:25:22.384197Z",
     "iopub.status.busy": "2025-12-02T17:25:22.383430Z",
     "iopub.status.idle": "2025-12-02T17:46:18.920269Z",
     "shell.execute_reply": "2025-12-02T17:46:18.919504Z"
    },
    "papermill": {
     "duration": 1256.605602,
     "end_time": "2025-12-02T17:46:18.921851",
     "exception": false,
     "start_time": "2025-12-02T17:25:22.316249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:25:26.262862: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764696326.282736     794 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764696326.288797     794 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 17:25:32 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\r\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\r\n",
      "2025-12-02 17:25:40.248287: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-12-02 17:25:40.254934: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764696340.268651     812 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764696340.274596     812 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764696340.275135     813 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764696340.282196     813 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 17:25:46 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "INFO 12-02 17:25:46 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "[Rank 1] Logical GPU count: 1\r\n",
      "[Rank 1] Loading dataset: amc -> math-ai/amc23\r\n",
      "[Rank 0] Logical GPU count: 1\r\n",
      "[Rank 0] Loading dataset: amc -> math-ai/amc23\r\n",
      "[Rank 0] Needs to process 20 prompts (indices [0, 20))\r\n",
      "[Rank 1] Needs to process 20 prompts (indices [20, 40))\r\n",
      "WARNING 12-02 17:25:56 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 12-02 17:25:56 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-02 17:26:21 [config.py:717] This model supports multiple tasks: {'embed', 'classify', 'reward', 'generate', 'score'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 17:26:21 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 17:26:21 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 17:26:21 [config.py:717] This model supports multiple tasks: {'score', 'embed', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 17:26:21 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 17:26:21 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 17:26:22 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 17:26:22 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 12-02 17:26:23 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 17:26:23 [cuda.py:289] Using XFormers backend.\r\n",
      "[W1202 17:26:23.661896638 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 17:26:23.662691095 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 17:26:23 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 17:26:23 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B...\r\n",
      "[W1202 17:26:23.710433770 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 17:26:23.711139512 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 17:26:23 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 17:26:23 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B...\r\n",
      "INFO 12-02 17:26:24 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 17:26:24 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 17:26:24 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "INFO 12-02 17:26:24 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.23s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.23s/it]\r\n",
      "\r\n",
      "INFO 12-02 17:26:27 [loader.py:458] Loading weights took 3.31 seconds\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.31s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.31s/it]\r\n",
      "\r\n",
      "INFO 12-02 17:26:27 [loader.py:458] Loading weights took 3.40 seconds\r\n",
      "INFO 12-02 17:26:28 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.739352 seconds\r\n",
      "INFO 12-02 17:26:28 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.960799 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 17:26:29 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 17:26:29 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 17:26:30 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 17:26:30 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 17:26:30 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 17:26:30 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 17:26:34 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 12-02 17:26:34 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.05it/s]\r\n",
      "INFO 12-02 17:27:07 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\r\n",
      "INFO 12-02 17:27:07 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.59 seconds\r\n",
      "[Rank 1] Starting generation for 20 prompts with batch_size=16\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.04it/s]\r\n",
      "INFO 12-02 17:27:08 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.19 GiB\r\n",
      "INFO 12-02 17:27:08 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 40.01 seconds\r\n",
      "[Rank 0] Starting generation for 20 prompts with batch_size=16\r\n",
      "Processed prompts:   6%| | 64/1024 [05:45<1:26:27,  5.40s/it, est. speed input: WARNING 12-02 17:33:18 [scheduler.py:1768] Sequence group 8_parallel_sample_54 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\r\n",
      "Processed prompts: 100%|█| 1024/1024 [12:31<00:00,  1.36it/s, est. speed input: \r\n",
      "[Rank 1] Processed 16/20 prompts with n=64 completions each\r\n",
      "Processed prompts: 100%|█| 1024/1024 [15:26<00:00,  1.11it/s, est. speed input: \r\n",
      "[Rank 0] Processed 16/20 prompts with n=64 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:21<00:00,  1.27it/s, est. speed input: 11\r\n",
      "[Rank 1] Processed 20/20 prompts with n=64 completions each\r\n",
      "[Rank 1] Saved partial JSONL to outputs/amc23_base_t1.0.rank1.jsonl\r\n",
      "[rank0]:[W1202 17:43:02.880031829 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Processed prompts: 100%|█| 256/256 [03:36<00:00,  1.18it/s, est. speed input: 10\r\n",
      "[Rank 0] Processed 20/20 prompts with n=64 completions each\r\n",
      "[Rank 0] Saved partial JSONL to outputs/amc23_base_t1.0.rank0.jsonl\r\n",
      "[rank0]:[W1202 17:46:14.130757261 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Total time consuming is: 1240.54s\r\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/amc23_base_t1.0.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B\" \\\n",
    "  --dataset \"amc\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 64 \\\n",
    "  --temperature 1.0 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/amc23_base_t1.0.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9485b0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:46:19.070263Z",
     "iopub.status.busy": "2025-12-02T17:46:19.069978Z",
     "iopub.status.idle": "2025-12-02T17:46:28.606627Z",
     "shell.execute_reply": "2025-12-02T17:46:28.605799Z"
    },
    "papermill": {
     "duration": 9.61108,
     "end_time": "2025-12-02T17:46:28.608023",
     "exception": false,
     "start_time": "2025-12-02T17:46:18.996943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/amc23_base_t1.0.jsonl...\r\n",
      "Scoring generations from outputs/amc23_base_t1.0.jsonl...\r\n",
      "Processing lines: 100%|████████████████████| 2560/2560 [00:08<00:00, 301.24it/s]\r\n",
      "Processing complete. Scored 2560 lines across 40 unique problems.\r\n",
      "\r\n",
      "Pass@k Metrics:\r\n",
      "  pass@1   : 25.70%\r\n",
      "  pass@2   : 40.05%\r\n",
      "  pass@4   : 55.36%\r\n",
      "  pass@8   : 68.97%\r\n",
      "  pass@16  : 79.66%\r\n",
      "  pass@32  : 87.80%\r\n",
      "  pass@64  : 95.00%\r\n",
      "\r\n",
      "Majority Vote Metric:\r\n",
      "  maj@1    : 42.50%\r\n",
      "\r\n",
      "Scored results saved to outputs/amc23_base_t1.0_eval.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py \\\n",
    "  --input_file outputs/amc23_base_t1.0.jsonl \\\n",
    "  --output_file outputs/amc23_base_t1.0_eval.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60a5da5",
   "metadata": {
    "papermill": {
     "duration": 0.073129,
     "end_time": "2025-12-02T17:46:28.755930",
     "exception": false,
     "start_time": "2025-12-02T17:46:28.682801",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Temperature = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "589c5dbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:46:28.904959Z",
     "iopub.status.busy": "2025-12-02T17:46:28.904240Z",
     "iopub.status.idle": "2025-12-02T18:07:07.510733Z",
     "shell.execute_reply": "2025-12-02T18:07:07.509992Z"
    },
    "papermill": {
     "duration": 1238.683431,
     "end_time": "2025-12-02T18:07:07.512255",
     "exception": false,
     "start_time": "2025-12-02T17:46:28.828824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:46:32.775057: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764697592.795447     965 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764697592.801643     965 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 17:46:38 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\r\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\r\n",
      "2025-12-02 17:46:46.613480: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764697606.633604     984 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2025-12-02 17:46:46.637034: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "E0000 00:00:1764697606.639759     984 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764697606.657986     983 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764697606.663938     983 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 17:46:52 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "INFO 12-02 17:46:52 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "[Rank 0] Logical GPU count: 1\r\n",
      "[Rank 0] Loading dataset: amc -> math-ai/amc23\r\n",
      "[Rank 1] Logical GPU count: 1\r\n",
      "[Rank 1] Loading dataset: amc -> math-ai/amc23\r\n",
      "[Rank 1] Needs to process 20 prompts (indices [20, 40))\r\n",
      "[Rank 0] Needs to process 20 prompts (indices [0, 20))\r\n",
      "WARNING 12-02 17:47:03 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 12-02 17:47:03 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-02 17:47:27 [config.py:717] This model supports multiple tasks: {'classify', 'embed', 'score', 'generate', 'reward'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 17:47:27 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 17:47:27 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 17:47:27 [config.py:717] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 17:47:27 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 17:47:27 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 17:47:29 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 17:47:29 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 12-02 17:47:29 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 17:47:29 [cuda.py:289] Using XFormers backend.\r\n",
      "[W1202 17:47:29.696067869 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 17:47:29.696779025 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 17:47:29 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 17:47:29 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B...\r\n",
      "[W1202 17:47:30.952755672 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 17:47:30.953439118 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 17:47:30 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 17:47:30 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B...\r\n",
      "INFO 12-02 17:47:30 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 17:47:30 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "INFO 12-02 17:47:30 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 17:47:30 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.24s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.24s/it]\r\n",
      "\r\n",
      "INFO 12-02 17:47:33 [loader.py:458] Loading weights took 3.32 seconds\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.33s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.34s/it]\r\n",
      "\r\n",
      "INFO 12-02 17:47:33 [loader.py:458] Loading weights took 3.41 seconds\r\n",
      "INFO 12-02 17:47:34 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.811414 seconds\r\n",
      "INFO 12-02 17:47:34 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.850133 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 17:47:35 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 17:47:35 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 17:47:36 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 17:47:36 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 17:47:36 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 17:47:36 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 17:47:40 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 12-02 17:47:40 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.06it/s]\r\n",
      "INFO 12-02 17:48:13 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\r\n",
      "INFO 12-02 17:48:13 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.39 seconds\r\n",
      "[Rank 1] Starting generation for 20 prompts with batch_size=16\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.05it/s]\r\n",
      "INFO 12-02 17:48:14 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\r\n",
      "INFO 12-02 17:48:14 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.70 seconds\r\n",
      "[Rank 0] Starting generation for 20 prompts with batch_size=16\r\n",
      "Processed prompts: 100%|█| 1024/1024 [13:52<00:00,  1.23it/s, est. speed input: \r\n",
      "[Rank 1] Processed 16/20 prompts with n=64 completions each\r\n",
      "Processed prompts: 100%|█| 1024/1024 [15:23<00:00,  1.11it/s, est. speed input: \r\n",
      "[Rank 0] Processed 16/20 prompts with n=64 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [03:12<00:00,  1.33it/s, est. speed input: 12\r\n",
      "[Rank 1] Processed 20/20 prompts with n=64 completions each\r\n",
      "[Rank 1] Saved partial JSONL to outputs/amc23_base_t1.2.rank1.jsonl\r\n",
      "[rank0]:[W1202 18:05:20.684854698 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Processed prompts: 100%|█| 256/256 [03:21<00:00,  1.27it/s, est. speed input: 10\r\n",
      "[Rank 0] Processed 20/20 prompts with n=64 completions each\r\n",
      "[Rank 0] Saved partial JSONL to outputs/amc23_base_t1.2.rank0.jsonl\r\n",
      "[rank0]:[W1202 18:07:02.597747773 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Total time consuming is: 1222.54s\r\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/amc23_base_t1.2.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B\" \\\n",
    "  --dataset \"amc\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 64 \\\n",
    "  --temperature 1.2 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/amc23_base_t1.2.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7312d9b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T18:07:07.674832Z",
     "iopub.status.busy": "2025-12-02T18:07:07.674554Z",
     "iopub.status.idle": "2025-12-02T18:07:18.975147Z",
     "shell.execute_reply": "2025-12-02T18:07:18.974167Z"
    },
    "papermill": {
     "duration": 11.383436,
     "end_time": "2025-12-02T18:07:18.977230",
     "exception": false,
     "start_time": "2025-12-02T18:07:07.593794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/amc23_base_t1.2.jsonl...\r\n",
      "Scoring generations from outputs/amc23_base_t1.2.jsonl...\r\n",
      "Processing lines:   6%|█▏                   | 141/2560 [00:00<00:08, 290.06it/s]Timeout during comparison\r\n",
      "Processing lines: 100%|████████████████████| 2560/2560 [00:10<00:00, 252.37it/s]\r\n",
      "Processing complete. Scored 2560 lines across 40 unique problems.\r\n",
      "\r\n",
      "Pass@k Metrics:\r\n",
      "  pass@1   : 14.30%\r\n",
      "  pass@2   : 24.84%\r\n",
      "  pass@4   : 39.23%\r\n",
      "  pass@8   : 55.01%\r\n",
      "  pass@16  : 68.90%\r\n",
      "  pass@32  : 79.57%\r\n",
      "  pass@64  : 87.50%\r\n",
      "\r\n",
      "Majority Vote Metric:\r\n",
      "  maj@1    : 10.00%\r\n",
      "\r\n",
      "Scored results saved to outputs/amc23_base_t1.2_eval.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py \\\n",
    "  --input_file outputs/amc23_base_t1.2.jsonl \\\n",
    "  --output_file outputs/amc23_base_t1.2_eval.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f078f2f",
   "metadata": {
    "papermill": {
     "duration": 0.080548,
     "end_time": "2025-12-02T18:07:19.141071",
     "exception": false,
     "start_time": "2025-12-02T18:07:19.060523",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### AIME25 Dataset (rollout-n = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6b5949",
   "metadata": {
    "papermill": {
     "duration": 0.080824,
     "end_time": "2025-12-02T18:07:19.302801",
     "exception": false,
     "start_time": "2025-12-02T18:07:19.221977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Temperature = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f81bd35d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T18:07:19.471164Z",
     "iopub.status.busy": "2025-12-02T18:07:19.470203Z",
     "iopub.status.idle": "2025-12-02T18:25:43.689382Z",
     "shell.execute_reply": "2025-12-02T18:25:43.688444Z"
    },
    "papermill": {
     "duration": 1104.307256,
     "end_time": "2025-12-02T18:25:43.690859",
     "exception": false,
     "start_time": "2025-12-02T18:07:19.383603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 18:07:23.289011: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764698843.309752    1136 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764698843.316043    1136 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 18:07:29 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\r\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\r\n",
      "2025-12-02 18:07:37.213373: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-12-02 18:07:37.228718: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764698857.234352    1154 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764698857.240585    1154 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764698857.249220    1155 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764698857.255592    1155 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 18:07:43 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "INFO 12-02 18:07:43 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "[Rank 1] Logical GPU count: 1\r\n",
      "[Rank 1] Loading dataset: aime -> math-ai/aime25\r\n",
      "[Rank 0] Logical GPU count: 1\r\n",
      "[Rank 0] Loading dataset: aime -> math-ai/aime25\r\n",
      "README.md: 100%|███████████████████████████████| 753/753 [00:00<00:00, 5.68MB/s]\r\n",
      "test.jsonl: 15.8kB [00:00, 46.9MB/s]\r\n",
      "Generating test split: 100%|███████████| 30/30 [00:00<00:00, 5514.23 examples/s]\r\n",
      "[Rank 0] Needs to process 15 prompts (indices [0, 15))\r\n",
      "[Rank 1] Needs to process 15 prompts (indices [15, 30))\r\n",
      "WARNING 12-02 18:07:53 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 12-02 18:07:53 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-02 18:08:18 [config.py:717] This model supports multiple tasks: {'embed', 'reward', 'classify', 'score', 'generate'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 18:08:18 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 18:08:18 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 18:08:18 [config.py:717] This model supports multiple tasks: {'classify', 'embed', 'score', 'generate', 'reward'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 18:08:18 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 18:08:18 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 18:08:20 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 18:08:20 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 12-02 18:08:20 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 18:08:20 [cuda.py:289] Using XFormers backend.\r\n",
      "[W1202 18:08:20.680936554 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 18:08:20.681793565 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 18:08:20 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 18:08:20 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B...\r\n",
      "[W1202 18:08:20.802275263 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 18:08:20.803059894 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 18:08:20 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 18:08:20 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B...\r\n",
      "INFO 12-02 18:08:21 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 18:08:21 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 18:08:21 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "INFO 12-02 18:08:21 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.26s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.26s/it]\r\n",
      "\r\n",
      "INFO 12-02 18:08:24 [loader.py:458] Loading weights took 3.34 seconds\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.36s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.36s/it]\r\n",
      "\r\n",
      "INFO 12-02 18:08:24 [loader.py:458] Loading weights took 3.44 seconds\r\n",
      "INFO 12-02 18:08:25 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.785201 seconds\r\n",
      "INFO 12-02 18:08:25 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.924392 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 18:08:26 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 18:08:26 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 18:08:27 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 18:08:27 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 18:08:27 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 18:08:27 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 18:08:31 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 12-02 18:08:31 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.04it/s]\r\n",
      "INFO 12-02 18:09:05 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.19 GiB\r\n",
      "INFO 12-02 18:09:05 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 40.16 seconds\r\n",
      "[Rank 1] Starting generation for 15 prompts with batch_size=16\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.03it/s]\r\n",
      "INFO 12-02 18:09:05 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.19 GiB\r\n",
      "INFO 12-02 18:09:05 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 40.43 seconds\r\n",
      "[Rank 0] Starting generation for 15 prompts with batch_size=16\r\n",
      "Processed prompts: 100%|█| 960/960 [15:27<00:00,  1.04it/s, est. speed input: 20\r\n",
      "[Rank 1] Processed 15/15 prompts with n=64 completions each\r\n",
      "[Rank 1] Saved partial JSONL to outputs/aime25_base_t0.6.rank1.jsonl\r\n",
      "[rank0]:[W1202 18:24:35.412603460 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Processed prompts: 100%|█| 960/960 [16:30<00:00,  1.03s/it, est. speed input: 19\r\n",
      "[Rank 0] Processed 15/15 prompts with n=64 completions each\r\n",
      "[Rank 0] Saved partial JSONL to outputs/aime25_base_t0.6.rank0.jsonl\r\n",
      "[rank0]:[W1202 18:25:38.857303346 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Total time consuming is: 1088.33s\r\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/aime25_base_t0.6.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B\" \\\n",
    "  --dataset \"aime\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 64 \\\n",
    "  --temperature 0.6 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/aime25_base_t0.6.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30c40088",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T18:25:43.869941Z",
     "iopub.status.busy": "2025-12-02T18:25:43.869355Z",
     "iopub.status.idle": "2025-12-02T18:25:51.770221Z",
     "shell.execute_reply": "2025-12-02T18:25:51.769523Z"
    },
    "papermill": {
     "duration": 7.990841,
     "end_time": "2025-12-02T18:25:51.771655",
     "exception": false,
     "start_time": "2025-12-02T18:25:43.780814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/aime25_base_t0.6.jsonl...\r\n",
      "Scoring generations from outputs/aime25_base_t0.6.jsonl...\r\n",
      "Processing lines:  83%|████████████████▌   | 1592/1920 [00:05<00:00, 340.28it/s]Timeout during comparison\r\n",
      "Processing lines: 100%|████████████████████| 1920/1920 [00:06<00:00, 277.58it/s]\r\n",
      "Processing complete. Scored 1920 lines across 30 unique problems.\r\n",
      "\r\n",
      "Pass@k Metrics:\r\n",
      "  pass@1   : 4.22%\r\n",
      "  pass@2   : 7.52%\r\n",
      "  pass@4   : 12.32%\r\n",
      "  pass@8   : 18.13%\r\n",
      "  pass@16  : 23.96%\r\n",
      "  pass@32  : 29.83%\r\n",
      "  pass@64  : 36.67%\r\n",
      "\r\n",
      "Majority Vote Metric:\r\n",
      "  maj@1    : 3.33%\r\n",
      "\r\n",
      "Scored results saved to outputs/aime25_base_t0.6_eval.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py \\\n",
    "  --input_file outputs/aime25_base_t0.6.jsonl \\\n",
    "  --output_file outputs/aime25_base_t0.6_eval.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e2ba16",
   "metadata": {
    "papermill": {
     "duration": 0.088154,
     "end_time": "2025-12-02T18:25:51.951947",
     "exception": false,
     "start_time": "2025-12-02T18:25:51.863793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2c6398b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T18:25:52.132286Z",
     "iopub.status.busy": "2025-12-02T18:25:52.131613Z",
     "iopub.status.idle": "2025-12-02T18:44:00.895597Z",
     "shell.execute_reply": "2025-12-02T18:44:00.894793Z"
    },
    "papermill": {
     "duration": 1088.856396,
     "end_time": "2025-12-02T18:44:00.897222",
     "exception": false,
     "start_time": "2025-12-02T18:25:52.040826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 18:25:56.053027: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764699956.075916    1313 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764699956.082738    1313 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 18:26:02 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\r\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\r\n",
      "2025-12-02 18:26:10.071690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764699970.093063    1332 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764699970.099208    1332 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-12-02 18:26:10.117218: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764699970.137850    1331 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764699970.143886    1331 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 18:26:16 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "INFO 12-02 18:26:16 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "[Rank 1] Logical GPU count: 1\r\n",
      "[Rank 1] Loading dataset: aime -> math-ai/aime25\r\n",
      "[Rank 0] Logical GPU count: 1\r\n",
      "[Rank 0] Loading dataset: aime -> math-ai/aime25\r\n",
      "[Rank 0] Needs to process 15 prompts (indices [0, 15))\r\n",
      "[Rank 1] Needs to process 15 prompts (indices [15, 30))\r\n",
      "WARNING 12-02 18:26:26 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 12-02 18:26:26 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-02 18:26:51 [config.py:717] This model supports multiple tasks: {'embed', 'classify', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 18:26:51 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 18:26:51 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 18:26:51 [config.py:717] This model supports multiple tasks: {'embed', 'score', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 18:26:51 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 18:26:51 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 18:26:53 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 18:26:53 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 12-02 18:26:53 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 18:26:53 [cuda.py:289] Using XFormers backend.\r\n",
      "[W1202 18:26:54.153841623 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 18:26:54.154561386 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 18:26:54.160635434 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 18:26:54.161368544 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 18:26:54 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 18:26:54 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B...\r\n",
      "INFO 12-02 18:26:54 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 18:26:54 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B...\r\n",
      "INFO 12-02 18:26:54 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 18:26:54 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 18:26:54 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "INFO 12-02 18:26:54 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.46s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.46s/it]\r\n",
      "\r\n",
      "INFO 12-02 18:26:57 [loader.py:458] Loading weights took 2.55 seconds\r\n",
      "INFO 12-02 18:26:57 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.064972 seconds\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.30s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.30s/it]\r\n",
      "\r\n",
      "INFO 12-02 18:26:58 [loader.py:458] Loading weights took 3.37 seconds\r\n",
      "INFO 12-02 18:26:58 [model_runner.py:1140] Model loading took 2.8798 GiB and 4.063812 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 18:26:59 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 18:26:59 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 18:26:59 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 18:27:00 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 18:27:00 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 18:27:00 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 18:27:04 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 12-02 18:27:05 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.05it/s]\r\n",
      "INFO 12-02 18:27:37 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\r\n",
      "INFO 12-02 18:27:37 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.57 seconds\r\n",
      "Capturing CUDA graph shapes:  97%|█████████████▌| 34/35 [00:32<00:00,  1.09it/s][Rank 0] Starting generation for 15 prompts with batch_size=16\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.04it/s]\r\n",
      "INFO 12-02 18:27:38 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.19 GiB\r\n",
      "INFO 12-02 18:27:38 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.99 seconds\r\n",
      "[Rank 1] Starting generation for 15 prompts with batch_size=16\r\n",
      "Processed prompts:   0%| | 0/960 [00:00<?, ?it/s, est. speed input: 0.00 toks/s,WARNING 12-02 18:33:53 [scheduler.py:1768] Sequence group 8_parallel_sample_25 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\r\n",
      "Processed prompts: 100%|█| 960/960 [15:30<00:00,  1.03it/s, est. speed input: 20\r\n",
      "[Rank 1] Processed 15/15 prompts with n=64 completions each\r\n",
      "[Rank 1] Saved partial JSONL to outputs/aime25_base_t1.0.rank1.jsonl\r\n",
      "[rank0]:[W1202 18:43:12.929822774 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Processed prompts: 100%|█| 960/960 [16:15<00:00,  1.02s/it, est. speed input: 19\r\n",
      "[Rank 0] Processed 15/15 prompts with n=64 completions each\r\n",
      "[Rank 0] Saved partial JSONL to outputs/aime25_base_t1.0.rank0.jsonl\r\n",
      "[rank0]:[W1202 18:43:56.077221667 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Total time consuming is: 1072.73s\r\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/aime25_base_t1.0.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B\" \\\n",
    "  --dataset \"aime\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 64 \\\n",
    "  --temperature 1.0 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/aime25_base_t1.0.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8044ed5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T18:44:01.088144Z",
     "iopub.status.busy": "2025-12-02T18:44:01.087565Z",
     "iopub.status.idle": "2025-12-02T18:44:09.944827Z",
     "shell.execute_reply": "2025-12-02T18:44:09.944107Z"
    },
    "papermill": {
     "duration": 8.954085,
     "end_time": "2025-12-02T18:44:09.946917",
     "exception": false,
     "start_time": "2025-12-02T18:44:00.992832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/aime25_base_t1.0.jsonl...\r\n",
      "Scoring generations from outputs/aime25_base_t1.0.jsonl...\r\n",
      "Processing lines: 100%|████████████████████| 1920/1920 [00:07<00:00, 244.93it/s]\r\n",
      "Processing complete. Scored 1920 lines across 30 unique problems.\r\n",
      "\r\n",
      "Pass@k Metrics:\r\n",
      "  pass@1   : 2.76%\r\n",
      "  pass@2   : 5.09%\r\n",
      "  pass@4   : 8.74%\r\n",
      "  pass@8   : 13.53%\r\n",
      "  pass@16  : 18.72%\r\n",
      "  pass@32  : 24.58%\r\n",
      "  pass@64  : 33.33%\r\n",
      "\r\n",
      "Majority Vote Metric:\r\n",
      "  maj@1    : 3.33%\r\n",
      "\r\n",
      "Scored results saved to outputs/aime25_base_t1.0_eval.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py \\\n",
    "  --input_file outputs/aime25_base_t1.0.jsonl \\\n",
    "  --output_file outputs/aime25_base_t1.0_eval.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec8397",
   "metadata": {
    "papermill": {
     "duration": 0.09621,
     "end_time": "2025-12-02T18:44:10.141009",
     "exception": false,
     "start_time": "2025-12-02T18:44:10.044799",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Temperature = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1e60a5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T18:44:10.334148Z",
     "iopub.status.busy": "2025-12-02T18:44:10.333880Z",
     "iopub.status.idle": "2025-12-02T19:01:58.482571Z",
     "shell.execute_reply": "2025-12-02T19:01:58.481623Z"
    },
    "papermill": {
     "duration": 1068.247983,
     "end_time": "2025-12-02T19:01:58.484015",
     "exception": false,
     "start_time": "2025-12-02T18:44:10.236032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 18:44:14.259936: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764701054.280094    1484 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764701054.286162    1484 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 18:44:20 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\r\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\r\n",
      "2025-12-02 18:44:28.303420: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-12-02 18:44:28.303666: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764701068.322754    1502 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764701068.323384    1503 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764701068.329618    1502 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "E0000 00:00:1764701068.330732    1503 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 18:44:34 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "INFO 12-02 18:44:34 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "[Rank 0] Logical GPU count: 1\r\n",
      "[Rank 0] Loading dataset: aime -> math-ai/aime25\r\n",
      "[Rank 1] Logical GPU count: 1\r\n",
      "[Rank 1] Loading dataset: aime -> math-ai/aime25\r\n",
      "[Rank 0] Needs to process 15 prompts (indices [0, 15))\r\n",
      "[Rank 1] Needs to process 15 prompts (indices [15, 30))\r\n",
      "WARNING 12-02 18:44:44 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 12-02 18:44:44 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-02 18:45:09 [config.py:717] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward', 'score'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 18:45:09 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 18:45:09 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 18:45:09 [config.py:717] This model supports multiple tasks: {'classify', 'generate', 'score', 'embed', 'reward'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 18:45:09 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 18:45:09 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 18:45:10 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 18:45:10 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 12-02 18:45:10 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 18:45:10 [cuda.py:289] Using XFormers backend.\r\n",
      "[W1202 18:45:11.667034867 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 18:45:11.667155372 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 18:45:11.667769507 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 18:45:11.667833154 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 18:45:11 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 18:45:11 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 18:45:11 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B...\r\n",
      "INFO 12-02 18:45:11 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B...\r\n",
      "INFO 12-02 18:45:12 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 18:45:12 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 18:45:12 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "INFO 12-02 18:45:12 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.43s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.43s/it]\r\n",
      "\r\n",
      "INFO 12-02 18:45:15 [loader.py:458] Loading weights took 3.52 seconds\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.46s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.46s/it]\r\n",
      "\r\n",
      "INFO 12-02 18:45:15 [loader.py:458] Loading weights took 3.55 seconds\r\n",
      "INFO 12-02 18:45:16 [model_runner.py:1140] Model loading took 2.8798 GiB and 4.034505 seconds\r\n",
      "INFO 12-02 18:45:16 [model_runner.py:1140] Model loading took 2.8798 GiB and 4.218427 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 18:45:17 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 18:45:18 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 18:45:18 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 18:45:18 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 18:45:18 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 18:45:18 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 18:45:22 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 12-02 18:45:22 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:34<00:00,  1.02it/s]\r\n",
      "INFO 12-02 18:45:56 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.19 GiB\r\n",
      "INFO 12-02 18:45:56 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 40.64 seconds\r\n",
      "[Rank 1] Starting generation for 15 prompts with batch_size=16\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:35<00:00,  1.01s/it]\r\n",
      "INFO 12-02 18:45:58 [model_runner.py:1592] Graph capturing finished in 35 secs, took 0.19 GiB\r\n",
      "INFO 12-02 18:45:58 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 41.66 seconds\r\n",
      "[Rank 0] Starting generation for 15 prompts with batch_size=16\r\n",
      "Processed prompts: 100%|█| 960/960 [15:07<00:00,  1.06it/s, est. speed input: 21\r\n",
      "[Rank 1] Processed 15/15 prompts with n=64 completions each\r\n",
      "[Rank 1] Saved partial JSONL to outputs/aime25_base_t1.2.rank1.jsonl\r\n",
      "Processed prompts:  80%|▊| 768/960 [15:06<02:36,  1.23it/s, est. speed input: 18[rank0]:[W1202 19:01:07.799899608 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Processed prompts: 100%|█| 960/960 [15:52<00:00,  1.01it/s, est. speed input: 20\r\n",
      "[Rank 0] Processed 15/15 prompts with n=64 completions each\r\n",
      "[Rank 0] Saved partial JSONL to outputs/aime25_base_t1.2.rank0.jsonl\r\n",
      "[rank0]:[W1202 19:01:53.530892412 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Total time consuming is: 1051.91s\r\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/aime25_base_t1.2.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B\" \\\n",
    "  --dataset \"aime\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 64 \\\n",
    "  --temperature 1.2 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/aime25_base_t1.2.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "313e7703",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T19:01:58.689168Z",
     "iopub.status.busy": "2025-12-02T19:01:58.688675Z",
     "iopub.status.idle": "2025-12-02T19:02:06.262119Z",
     "shell.execute_reply": "2025-12-02T19:02:06.261150Z"
    },
    "papermill": {
     "duration": 7.675545,
     "end_time": "2025-12-02T19:02:06.263522",
     "exception": false,
     "start_time": "2025-12-02T19:01:58.587977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/aime25_base_t1.2.jsonl...\r\n",
      "Scoring generations from outputs/aime25_base_t1.2.jsonl...\r\n",
      "Processing lines: 100%|████████████████████| 1920/1920 [00:06<00:00, 293.56it/s]\r\n",
      "Processing complete. Scored 1920 lines across 30 unique problems.\r\n",
      "\r\n",
      "Pass@k Metrics:\r\n",
      "  pass@1   : 1.25%\r\n",
      "  pass@2   : 2.43%\r\n",
      "  pass@4   : 4.62%\r\n",
      "  pass@8   : 8.38%\r\n",
      "  pass@16  : 14.10%\r\n",
      "  pass@32  : 21.66%\r\n",
      "  pass@64  : 30.00%\r\n",
      "\r\n",
      "Majority Vote Metric:\r\n",
      "  maj@1    : 0.00%\r\n",
      "\r\n",
      "Scored results saved to outputs/aime25_base_t1.2_eval.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py \\\n",
    "  --input_file outputs/aime25_base_t1.2.jsonl \\\n",
    "  --output_file outputs/aime25_base_t1.2_eval.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03c8002",
   "metadata": {
    "papermill": {
     "duration": 0.102543,
     "end_time": "2025-12-02T19:02:06.471346",
     "exception": false,
     "start_time": "2025-12-02T19:02:06.368803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## II. GRPO-tuned Model: Qwen/Qwen2.5-Math-1.5B-Instruct\n",
    "\n",
    "### Math500 Dataset (rollout-n = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d330a227",
   "metadata": {
    "papermill": {
     "duration": 0.103044,
     "end_time": "2025-12-02T19:02:06.756316",
     "exception": false,
     "start_time": "2025-12-02T19:02:06.653272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Temperature = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6aee54c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T19:02:06.965475Z",
     "iopub.status.busy": "2025-12-02T19:02:06.964722Z",
     "iopub.status.idle": "2025-12-02T19:32:52.305103Z",
     "shell.execute_reply": "2025-12-02T19:32:52.304032Z"
    },
    "papermill": {
     "duration": 1845.446359,
     "end_time": "2025-12-02T19:32:52.306584",
     "exception": false,
     "start_time": "2025-12-02T19:02:06.860225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 19:02:10.865303: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764702130.886301    1655 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764702130.892476    1655 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 19:02:16 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\r\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\r\n",
      "2025-12-02 19:02:24.794666: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-12-02 19:02:24.795231: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764702144.814831    1674 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764702144.817030    1673 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764702144.821019    1674 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "E0000 00:00:1764702144.823237    1673 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 19:02:31 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "INFO 12-02 19:02:31 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "[Rank 0] Logical GPU count: 1\r\n",
      "[Rank 0] Loading dataset: math -> math-ai/math500\r\n",
      "[Rank 1] Logical GPU count: 1\r\n",
      "[Rank 1] Loading dataset: math -> math-ai/math500\r\n",
      "tokenizer_config.json: 7.32kB [00:00, 26.4MB/s]\r\n",
      "vocab.json: 2.78MB [00:00, 51.1MB/s]\r\n",
      "merges.txt: 1.67MB [00:00, 133MB/s]\r\n",
      "tokenizer.json: 7.03MB [00:00, 183MB/s]\r\n",
      "[Rank 1] Needs to process 250 prompts (indices [250, 500))\r\n",
      "[Rank 0] Needs to process 250 prompts (indices [0, 250))\r\n",
      "config.json: 100%|█████████████████████████████| 656/656 [00:00<00:00, 3.42MB/s]\r\n",
      "WARNING 12-02 19:02:42 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 12-02 19:02:42 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-02 19:03:06 [config.py:717] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 19:03:06 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 19:03:06 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 19:03:06 [config.py:717] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 19:03:06 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 19:03:06 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "generation_config.json: 100%|██████████████████| 160/160 [00:00<00:00, 1.15MB/s]\r\n",
      "INFO 12-02 19:03:08 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 19:03:08 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 12-02 19:03:08 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 19:03:08 [cuda.py:289] Using XFormers backend.\r\n",
      "[W1202 19:03:09.034625185 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 19:03:09.035336062 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 19:03:09 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 19:03:09 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\r\n",
      "[W1202 19:03:09.096741352 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 19:03:09.097400589 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 19:03:09 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 19:03:09 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\r\n",
      "INFO 12-02 19:03:09 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 19:03:09 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "model.safetensors: 100%|████████████████████| 3.09G/3.09G [00:07<00:00, 398MB/s]\r\n",
      "INFO 12-02 19:03:17 [weight_utils.py:281] Time spent downloading weights for Qwen/Qwen2.5-Math-1.5B-Instruct: 7.985897 seconds\r\n",
      "INFO 12-02 19:03:17 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "INFO 12-02 19:03:17 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.10s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.10s/it]\r\n",
      "\r\n",
      "INFO 12-02 19:03:20 [loader.py:458] Loading weights took 3.16 seconds\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.37s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.37s/it]\r\n",
      "\r\n",
      "INFO 12-02 19:03:21 [model_runner.py:1140] Model loading took 2.8798 GiB and 11.504780 seconds\r\n",
      "INFO 12-02 19:03:21 [loader.py:458] Loading weights took 3.57 seconds\r\n",
      "INFO 12-02 19:03:21 [model_runner.py:1140] Model loading took 2.8798 GiB and 12.024288 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 19:03:22 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 19:03:23 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 19:03:23 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 19:03:23 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 19:03:23 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 19:03:23 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 19:03:28 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 12-02 19:03:29 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:35<00:00,  1.01s/it]\r\n",
      "INFO 12-02 19:04:04 [model_runner.py:1592] Graph capturing finished in 35 secs, took 0.19 GiB\r\n",
      "INFO 12-02 19:04:04 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 43.04 seconds\r\n",
      "[Rank 1] Starting generation for 250 prompts with batch_size=16\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:35<00:00,  1.01s/it]\r\n",
      "INFO 12-02 19:04:04 [model_runner.py:1592] Graph capturing finished in 35 secs, took 0.19 GiB\r\n",
      "INFO 12-02 19:04:04 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 43.12 seconds\r\n",
      "[Rank 0] Starting generation for 250 prompts with batch_size=16\r\n",
      "Processed prompts: 100%|█| 256/256 [01:07<00:00,  3.80it/s, est. speed input: 32\r\n",
      "[Rank 1] Processed 16/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:45<00:00,  2.42it/s, est. speed input: 29\r\n",
      "[Rank 0] Processed 16/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:09<00:00,  3.70it/s, est. speed input: 38\r\n",
      "[Rank 1] Processed 32/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:41<00:00,  2.53it/s, est. speed input: 24\r\n",
      "[Rank 0] Processed 32/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:28<00:00,  1.73it/s, est. speed input: 19\r\n",
      "[Rank 1] Processed 48/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:41<00:00,  2.53it/s, est. speed input: 19\r\n",
      "[Rank 0] Processed 48/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:58<00:00,  2.17it/s, est. speed input: 28\r\n",
      "[Rank 1] Processed 64/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:35<00:00,  2.67it/s, est. speed input: 19\r\n",
      "[Rank 0] Processed 64/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:34<00:00,  2.70it/s, est. speed input: 18\r\n",
      "[Rank 0] Processed 80/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:46<00:00,  2.40it/s, est. speed input: 23\r\n",
      "[Rank 1] Processed 80/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:40<00:00,  2.54it/s, est. speed input: 24\r\n",
      "[Rank 0] Processed 96/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:47<00:00,  2.37it/s, est. speed input: 17\r\n",
      "[Rank 1] Processed 96/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:49<00:00,  2.34it/s, est. speed input: 22\r\n",
      "[Rank 1] Processed 112/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:21<00:00,  1.81it/s, est. speed input: 23\r\n",
      "[Rank 0] Processed 112/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:38<00:00,  2.60it/s, est. speed input: 25\r\n",
      "[Rank 1] Processed 128/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:59<00:00,  2.14it/s, est. speed input: 18\r\n",
      "[Rank 0] Processed 128/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:43<00:00,  2.47it/s, est. speed input: 28\r\n",
      "[Rank 1] Processed 144/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:51<00:00,  2.29it/s, est. speed input: 19\r\n",
      "[Rank 0] Processed 144/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:50<00:00,  2.31it/s, est. speed input: 20\r\n",
      "[Rank 1] Processed 160/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:03<00:00,  2.08it/s, est. speed input: 23\r\n",
      "[Rank 0] Processed 160/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:05<00:00,  2.04it/s, est. speed input: 17\r\n",
      "[Rank 1] Processed 176/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:49<00:00,  2.34it/s, est. speed input: 28\r\n",
      "[Rank 0] Processed 176/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:59<00:00,  2.13it/s, est. speed input: 16\r\n",
      "[Rank 1] Processed 192/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:50<00:00,  2.33it/s, est. speed input: 27\r\n",
      "[Rank 0] Processed 192/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:43<00:00,  2.47it/s, est. speed input: 23\r\n",
      "[Rank 1] Processed 208/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:30<00:00,  2.83it/s, est. speed input: 24\r\n",
      "[Rank 0] Processed 208/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:25<00:00,  3.00it/s, est. speed input: 38\r\n",
      "[Rank 0] Processed 224/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:48<00:00,  2.36it/s, est. speed input: 26\r\n",
      "[Rank 1] Processed 224/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:08<00:00,  2.00it/s, est. speed input: 18\r\n",
      "[Rank 0] Processed 240/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:07<00:00,  2.02it/s, est. speed input: 19\r\n",
      "[Rank 1] Processed 240/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 160/160 [01:31<00:00,  1.76it/s, est. speed input: 17\r\n",
      "[Rank 1] Processed 250/250 prompts with n=16 completions each\r\n",
      "[Rank 1] Saved partial JSONL to outputs/math500_instruct_t0.6.rank1.jsonl\r\n",
      "[rank0]:[W1202 19:32:42.803161969 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Processed prompts: 100%|█| 160/160 [01:38<00:00,  1.63it/s, est. speed input: 16\r\n",
      "[Rank 0] Processed 250/250 prompts with n=16 completions each\r\n",
      "[Rank 0] Saved partial JSONL to outputs/math500_instruct_t0.6.rank0.jsonl\r\n",
      "[rank0]:[W1202 19:32:46.608156395 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Total time consuming is: 1829.35s\r\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/math500_instruct_t0.6.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B-Instruct\" \\\n",
    "  --dataset \"math\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 16 \\\n",
    "  --temperature 0.6 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/math500_instruct_t0.6.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b69a3d5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T19:32:52.567584Z",
     "iopub.status.busy": "2025-12-02T19:32:52.567292Z",
     "iopub.status.idle": "2025-12-02T19:33:20.052672Z",
     "shell.execute_reply": "2025-12-02T19:33:20.051530Z"
    },
    "papermill": {
     "duration": 27.615923,
     "end_time": "2025-12-02T19:33:20.054258",
     "exception": false,
     "start_time": "2025-12-02T19:32:52.438335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/math500_instruct_t0.6.jsonl...\r\n",
      "Scoring generations from outputs/math500_instruct_t0.6.jsonl...\r\n",
      "Processing lines: 100%|████████████████████| 8000/8000 [00:26<00:00, 302.71it/s]\r\n",
      "Processing complete. Scored 8000 lines across 500 unique problems.\r\n",
      "\r\n",
      "Pass@k Metrics:\r\n",
      "  pass@1   : 75.00%\r\n",
      "  pass@2   : 80.49%\r\n",
      "  pass@4   : 84.83%\r\n",
      "  pass@8   : 88.19%\r\n",
      "  pass@16  : 90.20%\r\n",
      "\r\n",
      "Majority Vote Metric:\r\n",
      "  maj@1    : 79.40%\r\n",
      "\r\n",
      "Scored results saved to outputs/math500_instruct_t0.6_eval.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py \\\n",
    "  --input_file outputs/math500_instruct_t0.6.jsonl \\\n",
    "  --output_file outputs/math500_instruct_t0.6_eval.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d54506",
   "metadata": {
    "papermill": {
     "duration": 0.136725,
     "end_time": "2025-12-02T19:33:20.330471",
     "exception": false,
     "start_time": "2025-12-02T19:33:20.193746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07d875b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T19:33:20.685149Z",
     "iopub.status.busy": "2025-12-02T19:33:20.684384Z",
     "iopub.status.idle": "2025-12-02T20:03:18.899077Z",
     "shell.execute_reply": "2025-12-02T20:03:18.898117Z"
    },
    "papermill": {
     "duration": 1798.433017,
     "end_time": "2025-12-02T20:03:18.900774",
     "exception": false,
     "start_time": "2025-12-02T19:33:20.467757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 19:33:25.149640: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764704005.171275    1859 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764704005.177607    1859 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 19:33:31 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\r\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\r\n",
      "2025-12-02 19:33:39.779487: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-12-02 19:33:39.779861: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764704019.799156    1877 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764704019.799774    1878 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764704019.806117    1877 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "E0000 00:00:1764704019.807176    1878 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 19:33:46 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "INFO 12-02 19:33:46 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "[Rank 0] Logical GPU count: 1\r\n",
      "[Rank 0] Loading dataset: math -> math-ai/math500\r\n",
      "[Rank 1] Logical GPU count: 1\r\n",
      "[Rank 1] Loading dataset: math -> math-ai/math500\r\n",
      "[Rank 1] Needs to process 250 prompts (indices [250, 500))\r\n",
      "[Rank 0] Needs to process 250 prompts (indices [0, 250))\r\n",
      "WARNING 12-02 19:33:56 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 12-02 19:33:56 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-02 19:34:20 [config.py:717] This model supports multiple tasks: {'classify', 'embed', 'generate', 'score', 'reward'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 19:34:20 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 19:34:20 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 19:34:20 [config.py:717] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward', 'score'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 19:34:20 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 19:34:20 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 19:34:22 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 19:34:22 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 12-02 19:34:22 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 19:34:22 [cuda.py:289] Using XFormers backend.\r\n",
      "[W1202 19:34:23.162461363 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 19:34:23.163220593 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 19:34:23 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 19:34:23 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\r\n",
      "[W1202 19:34:23.369526569 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 19:34:23.370232318 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 19:34:23 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 19:34:23 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\r\n",
      "INFO 12-02 19:34:23 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 19:34:23 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "INFO 12-02 19:34:23 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 19:34:23 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.29s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.29s/it]\r\n",
      "\r\n",
      "INFO 12-02 19:34:27 [loader.py:458] Loading weights took 3.38 seconds\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.38s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.38s/it]\r\n",
      "\r\n",
      "INFO 12-02 19:34:27 [loader.py:458] Loading weights took 3.46 seconds\r\n",
      "INFO 12-02 19:34:27 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.781898 seconds\r\n",
      "INFO 12-02 19:34:27 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.828045 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 19:34:29 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 19:34:29 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 19:34:29 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 19:34:29 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 19:34:29 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 19:34:29 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 19:34:33 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 12-02 19:34:34 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.05it/s]\r\n",
      "INFO 12-02 19:35:07 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\r\n",
      "INFO 12-02 19:35:07 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.82 seconds\r\n",
      "[Rank 1] Starting generation for 250 prompts with batch_size=16\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.04it/s]\r\n",
      "INFO 12-02 19:35:07 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.19 GiB\r\n",
      "INFO 12-02 19:35:07 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 40.20 seconds\r\n",
      "[Rank 0] Starting generation for 250 prompts with batch_size=16\r\n",
      "Processed prompts: 100%|█| 256/256 [01:04<00:00,  3.94it/s, est. speed input: 33\r\n",
      "[Rank 1] Processed 16/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:39<00:00,  2.58it/s, est. speed input: 31\r\n",
      "[Rank 0] Processed 16/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:08<00:00,  3.73it/s, est. speed input: 39\r\n",
      "[Rank 1] Processed 32/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:48<00:00,  2.36it/s, est. speed input: 22\r\n",
      "[Rank 0] Processed 32/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:16<00:00,  1.88it/s, est. speed input: 21\r\n",
      "[Rank 1] Processed 48/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:42<00:00,  2.51it/s, est. speed input: 19\r\n",
      "[Rank 0] Processed 48/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:55<00:00,  2.21it/s, est. speed input: 28\r\n",
      "[Rank 1] Processed 64/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:30<00:00,  2.82it/s, est. speed input: 21\r\n",
      "[Rank 0] Processed 64/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:42<00:00,  2.49it/s, est. speed input: 24\r\n",
      "[Rank 1] Processed 80/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:29<00:00,  2.85it/s, est. speed input: 19\r\n",
      "[Rank 0] Processed 80/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:46<00:00,  2.40it/s, est. speed input: 17\r\n",
      "[Rank 1] Processed 96/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:51<00:00,  2.30it/s, est. speed input: 22\r\n",
      "[Rank 0] Processed 96/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:39<00:00,  2.58it/s, est. speed input: 24\r\n",
      "[Rank 1] Processed 112/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:18<00:00,  1.85it/s, est. speed input: 24\r\n",
      "[Rank 0] Processed 112/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:22<00:00,  3.10it/s, est. speed input: 30\r\n",
      "[Rank 1] Processed 128/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:43<00:00,  2.48it/s, est. speed input: 21\r\n",
      "[Rank 0] Processed 128/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:42<00:00,  2.50it/s, est. speed input: 28\r\n",
      "[Rank 1] Processed 144/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:33<00:00,  2.74it/s, est. speed input: 23\r\n",
      "[Rank 0] Processed 144/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:48<00:00,  2.36it/s, est. speed input: 21\r\n",
      "[Rank 1] Processed 160/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:58<00:00,  2.16it/s, est. speed input: 24\r\n",
      "[Rank 0] Processed 160/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:03<00:00,  2.07it/s, est. speed input: 17\r\n",
      "[Rank 1] Processed 176/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:53<00:00,  2.26it/s, est. speed input: 27\r\n",
      "[Rank 0] Processed 176/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:55<00:00,  2.21it/s, est. speed input: 16\r\n",
      "[Rank 1] Processed 192/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:46<00:00,  2.40it/s, est. speed input: 28\r\n",
      "[Rank 0] Processed 192/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:44<00:00,  2.45it/s, est. speed input: 23\r\n",
      "[Rank 1] Processed 208/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:33<00:00,  2.73it/s, est. speed input: 23\r\n",
      "[Rank 0] Processed 208/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:45<00:00,  2.42it/s, est. speed input: 26\r\n",
      "[Rank 1] Processed 224/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:32<00:00,  2.78it/s, est. speed input: 36\r\n",
      "[Rank 0] Processed 224/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:58<00:00,  2.16it/s, est. speed input: 21\r\n",
      "[Rank 1] Processed 240/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:04<00:00,  2.06it/s, est. speed input: 19\r\n",
      "[Rank 0] Processed 240/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 160/160 [01:16<00:00,  2.09it/s, est. speed input: 20\r\n",
      "[Rank 1] Processed 250/250 prompts with n=16 completions each\r\n",
      "[Rank 1] Saved partial JSONL to outputs/math500_instruct_t1.0.rank1.jsonl\r\n",
      "Processed prompts:  30%|▎| 48/160 [00:48<01:31,  1.22it/s, est. speed input: 73.[rank0]:[W1202 20:02:23.737837452 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Processed prompts: 100%|█| 160/160 [01:37<00:00,  1.64it/s, est. speed input: 16\r\n",
      "[Rank 0] Processed 250/250 prompts with n=16 completions each\r\n",
      "[Rank 0] Saved partial JSONL to outputs/math500_instruct_t1.0.rank0.jsonl\r\n",
      "[rank0]:[W1202 20:03:14.039933331 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Total time consuming is: 1780.93s\r\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/math500_instruct_t1.0.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B-Instruct\" \\\n",
    "  --dataset \"math\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 16 \\\n",
    "  --temperature 1.0 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/math500_instruct_t1.0.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b7e0a61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T20:03:19.225080Z",
     "iopub.status.busy": "2025-12-02T20:03:19.224801Z",
     "iopub.status.idle": "2025-12-02T20:03:47.400154Z",
     "shell.execute_reply": "2025-12-02T20:03:47.399236Z"
    },
    "papermill": {
     "duration": 28.336599,
     "end_time": "2025-12-02T20:03:47.401707",
     "exception": false,
     "start_time": "2025-12-02T20:03:19.065108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/math500_instruct_t1.0.jsonl...\r\n",
      "Scoring generations from outputs/math500_instruct_t1.0.jsonl...\r\n",
      "Processing lines: 100%|████████████████████| 8000/8000 [00:27<00:00, 295.13it/s]\r\n",
      "Processing complete. Scored 8000 lines across 500 unique problems.\r\n",
      "\r\n",
      "Pass@k Metrics:\r\n",
      "  pass@1   : 74.42%\r\n",
      "  pass@2   : 80.90%\r\n",
      "  pass@4   : 85.77%\r\n",
      "  pass@8   : 89.32%\r\n",
      "  pass@16  : 92.20%\r\n",
      "\r\n",
      "Majority Vote Metric:\r\n",
      "  maj@1    : 80.40%\r\n",
      "\r\n",
      "Scored results saved to outputs/math500_instruct_t1.0_eval.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py \\\n",
    "  --input_file outputs/math500_instruct_t1.0.jsonl \\\n",
    "  --output_file outputs/math500_instruct_t1.0_eval.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a6330c",
   "metadata": {
    "papermill": {
     "duration": 0.16623,
     "end_time": "2025-12-02T20:03:47.743233",
     "exception": false,
     "start_time": "2025-12-02T20:03:47.577003",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Temperature = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17fe04f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T20:03:48.114075Z",
     "iopub.status.busy": "2025-12-02T20:03:48.113798Z",
     "iopub.status.idle": "2025-12-02T20:38:48.520849Z",
     "shell.execute_reply": "2025-12-02T20:38:48.520018Z"
    },
    "papermill": {
     "duration": 2100.605316,
     "end_time": "2025-12-02T20:38:48.522288",
     "exception": false,
     "start_time": "2025-12-02T20:03:47.916972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 20:03:51.976798: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764705831.997006    2030 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764705832.003280    2030 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 20:03:58 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\r\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\r\n",
      "2025-12-02 20:04:05.896110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-12-02 20:04:05.896318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764705845.917705    2048 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764705845.917714    2049 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764705845.924124    2049 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "E0000 00:00:1764705845.924147    2048 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 20:04:12 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "INFO 12-02 20:04:12 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "[Rank 1] Logical GPU count: 1\r\n",
      "[Rank 1] Loading dataset: math -> math-ai/math500\r\n",
      "[Rank 0] Logical GPU count: 1\r\n",
      "[Rank 0] Loading dataset: math -> math-ai/math500\r\n",
      "[Rank 1] Needs to process 250 prompts (indices [250, 500))\r\n",
      "[Rank 0] Needs to process 250 prompts (indices [0, 250))\r\n",
      "WARNING 12-02 20:04:22 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 12-02 20:04:22 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-02 20:04:46 [config.py:717] This model supports multiple tasks: {'classify', 'generate', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 20:04:46 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 20:04:46 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 20:04:46 [config.py:717] This model supports multiple tasks: {'classify', 'embed', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 20:04:46 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 20:04:46 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 20:04:48 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 20:04:48 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 12-02 20:04:48 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 20:04:48 [cuda.py:289] Using XFormers backend.\r\n",
      "[W1202 20:04:49.103749849 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 20:04:49.104488627 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 20:04:49 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 20:04:49 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\r\n",
      "[W1202 20:04:49.202845576 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 20:04:49.203587781 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 20:04:49 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 20:04:49 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\r\n",
      "INFO 12-02 20:04:49 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 20:04:49 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 20:04:49 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "INFO 12-02 20:04:49 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.30s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.30s/it]\r\n",
      "\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.24s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.24s/it]\r\n",
      "\r\n",
      "INFO 12-02 20:04:52 [loader.py:458] Loading weights took 3.38 seconds\r\n",
      "INFO 12-02 20:04:53 [loader.py:458] Loading weights took 3.34 seconds\r\n",
      "INFO 12-02 20:04:53 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.771248 seconds\r\n",
      "INFO 12-02 20:04:53 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.737948 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 20:04:55 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 20:04:55 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 20:04:55 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 20:04:55 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 20:04:55 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 20:04:55 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 20:04:59 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "INFO 12-02 20:04:59 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.05it/s]\r\n",
      "INFO 12-02 20:05:33 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\r\n",
      "INFO 12-02 20:05:33 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.85 seconds\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.05it/s]\r\n",
      "INFO 12-02 20:05:33 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\r\n",
      "INFO 12-02 20:05:33 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.81 seconds\r\n",
      "[Rank 1] Starting generation for 250 prompts with batch_size=16\r\n",
      "[Rank 0] Starting generation for 250 prompts with batch_size=16\r\n",
      "Processed prompts: 100%|█| 256/256 [01:41<00:00,  2.52it/s, est. speed input: 21\r\n",
      "[Rank 1] Processed 16/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:10<00:00,  1.96it/s, est. speed input: 23\r\n",
      "[Rank 0] Processed 16/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:08<00:00,  1.99it/s, est. speed input: 20\r\n",
      "[Rank 1] Processed 32/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:20<00:00,  1.83it/s, est. speed input: 17\r\n",
      "[Rank 0] Processed 32/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:57<00:00,  2.19it/s, est. speed input: 16\r\n",
      "[Rank 0] Processed 48/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:37<00:00,  1.62it/s, est. speed input: 18\r\n",
      "[Rank 1] Processed 48/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:46<00:00,  2.41it/s, est. speed input: 18\r\n",
      "[Rank 0] Processed 64/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:02<00:00,  2.10it/s, est. speed input: 27\r\n",
      "[Rank 1] Processed 64/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:35<00:00,  2.69it/s, est. speed input: 18\r\n",
      "[Rank 0] Processed 80/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:57<00:00,  2.19it/s, est. speed input: 21\r\n",
      "[Rank 1] Processed 80/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:01<00:00,  2.11it/s, est. speed input: 20\r\n",
      "[Rank 0] Processed 96/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:50<00:00,  2.31it/s, est. speed input: 16\r\n",
      "[Rank 1] Processed 96/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:01<00:00,  2.11it/s, est. speed input: 20\r\n",
      "[Rank 1] Processed 112/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:33<00:00,  1.67it/s, est. speed input: 21\r\n",
      "[Rank 0] Processed 112/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:49<00:00,  2.33it/s, est. speed input: 22\r\n",
      "[Rank 1] Processed 128/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:10<00:00,  1.96it/s, est. speed input: 17\r\n",
      "[Rank 0] Processed 128/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:21<00:00,  1.82it/s, est. speed input: 20\r\n",
      "[Rank 1] Processed 144/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:12<00:00,  1.93it/s, est. speed input: 16\r\n",
      "[Rank 0] Processed 144/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:11<00:00,  1.94it/s, est. speed input: 17\r\n",
      "[Rank 1] Processed 160/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:27<00:00,  1.74it/s, est. speed input: 19\r\n",
      "[Rank 0] Processed 160/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:22<00:00,  1.80it/s, est. speed input: 15\r\n",
      "[Rank 1] Processed 176/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:00<00:00,  2.13it/s, est. speed input: 25\r\n",
      "[Rank 0] Processed 176/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:57<00:00,  2.19it/s, est. speed input: 16\r\n",
      "[Rank 1] Processed 192/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:56<00:00,  2.20it/s, est. speed input: 26\r\n",
      "[Rank 0] Processed 192/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:56<00:00,  2.19it/s, est. speed input: 20\r\n",
      "[Rank 1] Processed 208/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:51<00:00,  2.29it/s, est. speed input: 19\r\n",
      "[Rank 0] Processed 208/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [01:59<00:00,  2.14it/s, est. speed input: 23\r\n",
      "[Rank 1] Processed 224/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:01<00:00,  2.10it/s, est. speed input: 27\r\n",
      "[Rank 0] Processed 224/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:06<00:00,  2.03it/s, est. speed input: 19\r\n",
      "[Rank 0] Processed 240/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:16<00:00,  1.87it/s, est. speed input: 18\r\n",
      "[Rank 1] Processed 240/250 prompts with n=16 completions each\r\n",
      "Processed prompts: 100%|█| 160/160 [01:46<00:00,  1.50it/s, est. speed input: 14\r\n",
      "[Rank 1] Processed 250/250 prompts with n=16 completions each\r\n",
      "[Rank 1] Saved partial JSONL to outputs/math500_instruct_t1.2.rank1.jsonl\r\n",
      "[rank0]:[W1202 20:38:39.510912603 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Processed prompts: 100%|█| 160/160 [01:55<00:00,  1.39it/s, est. speed input: 13\r\n",
      "[Rank 0] Processed 250/250 prompts with n=16 completions each\r\n",
      "[Rank 0] Saved partial JSONL to outputs/math500_instruct_t1.2.rank0.jsonl\r\n",
      "[rank0]:[W1202 20:38:43.648513673 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Total time consuming is: 2084.29s\r\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/math500_instruct_t1.2.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B-Instruct\" \\\n",
    "  --dataset \"math\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 16 \\\n",
    "  --temperature 1.2 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/math500_instruct_t1.2.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1be9ac88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T20:38:48.983528Z",
     "iopub.status.busy": "2025-12-02T20:38:48.983231Z",
     "iopub.status.idle": "2025-12-02T20:39:16.393203Z",
     "shell.execute_reply": "2025-12-02T20:39:16.392489Z"
    },
    "papermill": {
     "duration": 27.599912,
     "end_time": "2025-12-02T20:39:16.394990",
     "exception": false,
     "start_time": "2025-12-02T20:38:48.795078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/math500_instruct_t1.2.jsonl...\r\n",
      "Scoring generations from outputs/math500_instruct_t1.2.jsonl...\r\n",
      "Processing lines: 100%|████████████████████| 8000/8000 [00:26<00:00, 304.37it/s]\r\n",
      "Processing complete. Scored 8000 lines across 500 unique problems.\r\n",
      "\r\n",
      "Pass@k Metrics:\r\n",
      "  pass@1   : 70.20%\r\n",
      "  pass@2   : 77.66%\r\n",
      "  pass@4   : 83.00%\r\n",
      "  pass@8   : 87.04%\r\n",
      "  pass@16  : 90.20%\r\n",
      "\r\n",
      "Majority Vote Metric:\r\n",
      "  maj@1    : 76.40%\r\n",
      "\r\n",
      "Scored results saved to outputs/math500_instruct_t1.2_eval.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py \\\n",
    "  --input_file outputs/math500_instruct_t1.2.jsonl \\\n",
    "  --output_file outputs/math500_instruct_t1.2_eval.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee362717",
   "metadata": {
    "papermill": {
     "duration": 0.195465,
     "end_time": "2025-12-02T20:39:16.791609",
     "exception": false,
     "start_time": "2025-12-02T20:39:16.596144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### AMC23 Dataset (rollout-n = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a584ca",
   "metadata": {
    "papermill": {
     "duration": 0.19183,
     "end_time": "2025-12-02T20:39:17.175029",
     "exception": false,
     "start_time": "2025-12-02T20:39:16.983199",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Temperature = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68d67deb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T20:39:17.567832Z",
     "iopub.status.busy": "2025-12-02T20:39:17.567069Z",
     "iopub.status.idle": "2025-12-02T20:52:56.559420Z",
     "shell.execute_reply": "2025-12-02T20:52:56.558658Z"
    },
    "papermill": {
     "duration": 819.190854,
     "end_time": "2025-12-02T20:52:56.560864",
     "exception": false,
     "start_time": "2025-12-02T20:39:17.370010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 20:39:21.475100: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764707961.495030    2201 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764707961.501076    2201 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 20:39:27 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\r\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\r\n",
      "2025-12-02 20:39:35.332825: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-12-02 20:39:35.336208: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764707975.353148    2220 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764707975.356019    2219 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764707975.360640    2220 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "E0000 00:00:1764707975.362561    2219 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 20:39:41 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "INFO 12-02 20:39:41 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "[Rank 1] Logical GPU count: 1\r\n",
      "[Rank 1] Loading dataset: amc -> math-ai/amc23\r\n",
      "[Rank 0] Logical GPU count: 1\r\n",
      "[Rank 0] Loading dataset: amc -> math-ai/amc23\r\n",
      "[Rank 0] Needs to process 20 prompts (indices [0, 20))\r\n",
      "[Rank 1] Needs to process 20 prompts (indices [20, 40))\r\n",
      "WARNING 12-02 20:39:51 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 12-02 20:39:51 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-02 20:40:15 [config.py:717] This model supports multiple tasks: {'embed', 'generate', 'classify', 'reward', 'score'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 20:40:15 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 20:40:15 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 20:40:16 [config.py:717] This model supports multiple tasks: {'score', 'generate', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 20:40:16 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 20:40:16 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 20:40:17 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 20:40:17 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 12-02 20:40:17 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 20:40:17 [cuda.py:289] Using XFormers backend.\r\n",
      "[W1202 20:40:18.506110906 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 20:40:18.506841233 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 20:40:18.506880309 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 20:40:18.507507808 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 20:40:18 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 20:40:18 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\r\n",
      "INFO 12-02 20:40:18 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 20:40:18 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\r\n",
      "INFO 12-02 20:40:18 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 20:40:18 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 20:40:19 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "INFO 12-02 20:40:19 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.31s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.31s/it]\r\n",
      "\r\n",
      "INFO 12-02 20:40:22 [loader.py:458] Loading weights took 3.38 seconds\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.35s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.35s/it]\r\n",
      "\r\n",
      "INFO 12-02 20:40:22 [loader.py:458] Loading weights took 3.44 seconds\r\n",
      "INFO 12-02 20:40:23 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.911293 seconds\r\n",
      "INFO 12-02 20:40:23 [model_runner.py:1140] Model loading took 2.8798 GiB and 4.119466 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 20:40:24 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 20:40:24 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 20:40:25 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 20:40:25 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 20:40:25 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 20:40:25 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 20:40:29 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 12-02 20:40:29 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.05it/s]\r\n",
      "INFO 12-02 20:41:02 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\r\n",
      "INFO 12-02 20:41:02 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.66 seconds\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.04it/s]\r\n",
      "INFO 12-02 20:41:02 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.19 GiB\r\n",
      "INFO 12-02 20:41:02 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.94 seconds\r\n",
      "[Rank 1] Starting generation for 20 prompts with batch_size=16\r\n",
      "[Rank 0] Starting generation for 20 prompts with batch_size=16\r\n",
      "Processed prompts: 100%|█| 1024/1024 [09:08<00:00,  1.87it/s, est. speed input: \r\n",
      "[Rank 1] Processed 16/20 prompts with n=64 completions each\r\n",
      "Processed prompts: 100%|█| 1024/1024 [09:18<00:00,  1.83it/s, est. speed input: \r\n",
      "[Rank 0] Processed 16/20 prompts with n=64 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:05<00:00,  2.04it/s, est. speed input: 17\r\n",
      "[Rank 0] Processed 20/20 prompts with n=64 completions each\r\n",
      "[Rank 0] Saved partial JSONL to outputs/amc23_instruct_t0.6.rank0.jsonl\r\n",
      "[rank0]:[W1202 20:52:29.755453442 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Processed prompts: 100%|█| 256/256 [02:37<00:00,  1.63it/s, est. speed input: 14\r\n",
      "[Rank 1] Processed 20/20 prompts with n=64 completions each\r\n",
      "[Rank 1] Saved partial JSONL to outputs/amc23_instruct_t0.6.rank1.jsonl\r\n",
      "[rank0]:[W1202 20:52:51.686110827 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Total time consuming is: 802.91s\r\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/amc23_instruct_t0.6.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B-Instruct\" \\\n",
    "  --dataset \"amc\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 64 \\\n",
    "  --temperature 0.6 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/amc23_instruct_t0.6.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23cc9b7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T20:52:56.966290Z",
     "iopub.status.busy": "2025-12-02T20:52:56.965642Z",
     "iopub.status.idle": "2025-12-02T20:53:03.748296Z",
     "shell.execute_reply": "2025-12-02T20:53:03.747589Z"
    },
    "papermill": {
     "duration": 6.986246,
     "end_time": "2025-12-02T20:53:03.749611",
     "exception": false,
     "start_time": "2025-12-02T20:52:56.763365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/amc23_instruct_t0.6.jsonl...\r\n",
      "Scoring generations from outputs/amc23_instruct_t0.6.jsonl...\r\n",
      "Processing lines: 100%|████████████████████| 2560/2560 [00:05<00:00, 438.46it/s]\r\n",
      "Processing complete. Scored 2560 lines across 40 unique problems.\r\n",
      "\r\n",
      "Pass@k Metrics:\r\n",
      "  pass@1   : 53.36%\r\n",
      "  pass@2   : 63.30%\r\n",
      "  pass@4   : 70.77%\r\n",
      "  pass@8   : 76.37%\r\n",
      "  pass@16  : 81.60%\r\n",
      "  pass@32  : 86.64%\r\n",
      "  pass@64  : 92.50%\r\n",
      "\r\n",
      "Majority Vote Metric:\r\n",
      "  maj@1    : 67.50%\r\n",
      "\r\n",
      "Scored results saved to outputs/amc23_instruct_t0.6_eval.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py \\\n",
    "  --input_file outputs/amc23_instruct_t0.6.jsonl \\\n",
    "  --output_file outputs/amc23_instruct_t0.6_eval.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52ecea4",
   "metadata": {
    "papermill": {
     "duration": 0.201399,
     "end_time": "2025-12-02T20:53:04.154595",
     "exception": false,
     "start_time": "2025-12-02T20:53:03.953196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fab51271",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T20:53:04.560621Z",
     "iopub.status.busy": "2025-12-02T20:53:04.559864Z",
     "iopub.status.idle": "2025-12-02T21:06:00.902926Z",
     "shell.execute_reply": "2025-12-02T21:06:00.902058Z"
    },
    "papermill": {
     "duration": 776.548343,
     "end_time": "2025-12-02T21:06:00.904318",
     "exception": false,
     "start_time": "2025-12-02T20:53:04.355975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 20:53:08.444092: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764708788.465115    2372 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764708788.470933    2372 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 20:53:14 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\r\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\r\n",
      "2025-12-02 20:53:22.223281: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-12-02 20:53:22.223453: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764708802.242843    2391 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764708802.243336    2390 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764708802.249124    2391 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "E0000 00:00:1764708802.249631    2390 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 20:53:28 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "INFO 12-02 20:53:28 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "[Rank 1] Logical GPU count: 1\r\n",
      "[Rank 1] Loading dataset: amc -> math-ai/amc23\r\n",
      "[Rank 0] Logical GPU count: 1\r\n",
      "[Rank 0] Loading dataset: amc -> math-ai/amc23\r\n",
      "[Rank 1] Needs to process 20 prompts (indices [20, 40))\r\n",
      "[Rank 0] Needs to process 20 prompts (indices [0, 20))\r\n",
      "WARNING 12-02 20:53:38 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 12-02 20:53:38 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-02 20:54:02 [config.py:717] This model supports multiple tasks: {'classify', 'reward', 'embed', 'generate', 'score'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 20:54:02 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 20:54:02 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 20:54:02 [config.py:717] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate', 'score'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 20:54:02 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 20:54:02 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 20:54:04 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 20:54:04 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 12-02 20:54:04 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 20:54:04 [cuda.py:289] Using XFormers backend.\r\n",
      "[W1202 20:54:05.553159789 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 20:54:05.553753650 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 20:54:05.553870589 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 20:54:05.554391351 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 20:54:05 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 20:54:05 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 20:54:05 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\r\n",
      "INFO 12-02 20:54:05 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\r\n",
      "INFO 12-02 20:54:05 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 20:54:05 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 20:54:06 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "INFO 12-02 20:54:06 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.39s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.39s/it]\r\n",
      "\r\n",
      "INFO 12-02 20:54:09 [loader.py:458] Loading weights took 3.47 seconds\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.50s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.50s/it]\r\n",
      "\r\n",
      "INFO 12-02 20:54:09 [loader.py:458] Loading weights took 3.61 seconds\r\n",
      "INFO 12-02 20:54:10 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.894714 seconds\r\n",
      "INFO 12-02 20:54:10 [model_runner.py:1140] Model loading took 2.8798 GiB and 4.179761 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 20:54:11 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 20:54:11 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 20:54:12 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 20:54:12 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 20:54:12 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 20:54:12 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 20:54:16 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 12-02 20:54:16 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.04it/s]\r\n",
      "INFO 12-02 20:54:50 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.19 GiB\r\n",
      "INFO 12-02 20:54:50 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.93 seconds\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.04it/s]\r\n",
      "INFO 12-02 20:54:50 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.19 GiB\r\n",
      "INFO 12-02 20:54:50 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 40.15 seconds\r\n",
      "[Rank 1] Starting generation for 20 prompts with batch_size=16\r\n",
      "[Rank 0] Starting generation for 20 prompts with batch_size=16\r\n",
      "Processed prompts: 100%|█| 1024/1024 [08:32<00:00,  2.00it/s, est. speed input: \r\n",
      "[Rank 1] Processed 16/20 prompts with n=64 completions each\r\n",
      "Processed prompts: 100%|█| 1024/1024 [08:47<00:00,  1.94it/s, est. speed input: \r\n",
      "[Rank 0] Processed 16/20 prompts with n=64 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:08<00:00,  1.99it/s, est. speed input: 17\r\n",
      "[Rank 0] Processed 20/20 prompts with n=64 completions each\r\n",
      "[Rank 0] Saved partial JSONL to outputs/amc23_instruct_t1.0.rank0.jsonl\r\n",
      "[rank0]:[W1202 21:05:49.127809867 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Processed prompts: 100%|█| 256/256 [02:30<00:00,  1.70it/s, est. speed input: 15\r\n",
      "[Rank 1] Processed 20/20 prompts with n=64 completions each\r\n",
      "[Rank 1] Saved partial JSONL to outputs/amc23_instruct_t1.0.rank1.jsonl\r\n",
      "[rank0]:[W1202 21:05:56.095497606 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Total time consuming is: 760.43s\r\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/amc23_instruct_t1.0.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B-Instruct\" \\\n",
    "  --dataset \"amc\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 64 \\\n",
    "  --temperature 1.0 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/amc23_instruct_t1.0.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28977070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T21:06:01.323579Z",
     "iopub.status.busy": "2025-12-02T21:06:01.322860Z",
     "iopub.status.idle": "2025-12-02T21:06:08.329066Z",
     "shell.execute_reply": "2025-12-02T21:06:08.328368Z"
    },
    "papermill": {
     "duration": 7.217429,
     "end_time": "2025-12-02T21:06:08.330922",
     "exception": false,
     "start_time": "2025-12-02T21:06:01.113493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/amc23_instruct_t1.0.jsonl...\r\n",
      "Scoring generations from outputs/amc23_instruct_t1.0.jsonl...\r\n",
      "Processing lines: 100%|████████████████████| 2560/2560 [00:06<00:00, 422.69it/s]\r\n",
      "Processing complete. Scored 2560 lines across 40 unique problems.\r\n",
      "\r\n",
      "Pass@k Metrics:\r\n",
      "  pass@1   : 52.27%\r\n",
      "  pass@2   : 63.47%\r\n",
      "  pass@4   : 72.55%\r\n",
      "  pass@8   : 79.53%\r\n",
      "  pass@16  : 85.08%\r\n",
      "  pass@32  : 88.71%\r\n",
      "  pass@64  : 90.00%\r\n",
      "\r\n",
      "Majority Vote Metric:\r\n",
      "  maj@1    : 65.00%\r\n",
      "\r\n",
      "Scored results saved to outputs/amc23_instruct_t1.0_eval.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py \\\n",
    "  --input_file outputs/amc23_instruct_t1.0.jsonl \\\n",
    "  --output_file outputs/amc23_instruct_t1.0_eval.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed38ddb",
   "metadata": {
    "papermill": {
     "duration": 0.220785,
     "end_time": "2025-12-02T21:06:08.764587",
     "exception": false,
     "start_time": "2025-12-02T21:06:08.543802",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Temperature = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cee1aed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T21:06:09.192885Z",
     "iopub.status.busy": "2025-12-02T21:06:09.192196Z",
     "iopub.status.idle": "2025-12-02T21:21:42.232997Z",
     "shell.execute_reply": "2025-12-02T21:21:42.231919Z"
    },
    "papermill": {
     "duration": 933.255874,
     "end_time": "2025-12-02T21:21:42.234685",
     "exception": false,
     "start_time": "2025-12-02T21:06:08.978811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 21:06:13.064745: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764709573.084338    2543 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764709573.090203    2543 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 21:06:19 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\r\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\r\n",
      "2025-12-02 21:06:26.921790: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764709586.942238    2561 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764709586.948288    2561 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-12-02 21:06:26.975430: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764709586.995691    2562 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764709587.001528    2562 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 21:06:33 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "INFO 12-02 21:06:33 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "[Rank 0] Logical GPU count: 1\r\n",
      "[Rank 0] Loading dataset: amc -> math-ai/amc23\r\n",
      "[Rank 1] Logical GPU count: 1\r\n",
      "[Rank 1] Loading dataset: amc -> math-ai/amc23\r\n",
      "[Rank 1] Needs to process 20 prompts (indices [20, 40))\r\n",
      "[Rank 0] Needs to process 20 prompts (indices [0, 20))\r\n",
      "WARNING 12-02 21:06:43 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 12-02 21:06:43 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-02 21:07:08 [config.py:717] This model supports multiple tasks: {'generate', 'reward', 'score', 'embed', 'classify'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 21:07:08 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 21:07:08 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 21:07:08 [config.py:717] This model supports multiple tasks: {'classify', 'generate', 'embed', 'score', 'reward'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 21:07:08 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 21:07:08 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 21:07:09 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 21:07:09 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 12-02 21:07:10 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 21:07:10 [cuda.py:289] Using XFormers backend.\r\n",
      "[W1202 21:07:10.512895112 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 21:07:10.513617020 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 21:07:10 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 21:07:10 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\r\n",
      "[W1202 21:07:10.764450104 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 21:07:10.765229354 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 21:07:10 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 21:07:10 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\r\n",
      "INFO 12-02 21:07:10 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 21:07:11 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "INFO 12-02 21:07:11 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 21:07:11 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.39s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.40s/it]\r\n",
      "\r\n",
      "INFO 12-02 21:07:14 [loader.py:458] Loading weights took 3.48 seconds\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.46s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.46s/it]\r\n",
      "\r\n",
      "INFO 12-02 21:07:14 [loader.py:458] Loading weights took 3.57 seconds\r\n",
      "INFO 12-02 21:07:15 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.932710 seconds\r\n",
      "INFO 12-02 21:07:15 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.989705 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 21:07:16 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 21:07:16 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 21:07:17 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 21:07:17 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 21:07:17 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 21:07:17 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 21:07:21 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 12-02 21:07:21 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.05it/s]\r\n",
      "INFO 12-02 21:07:54 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\r\n",
      "INFO 12-02 21:07:54 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.47 seconds\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.05it/s]\r\n",
      "INFO 12-02 21:07:54 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\r\n",
      "INFO 12-02 21:07:54 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 39.67 seconds\r\n",
      "[Rank 1] Starting generation for 20 prompts with batch_size=16\r\n",
      "[Rank 0] Starting generation for 20 prompts with batch_size=16\r\n",
      "Processed prompts: 100%|█| 1024/1024 [10:12<00:00,  1.67it/s, est. speed input: \r\n",
      "[Rank 0] Processed 16/20 prompts with n=64 completions each\r\n",
      "Processed prompts: 100%|█| 1024/1024 [11:01<00:00,  1.55it/s, est. speed input: \r\n",
      "[Rank 1] Processed 16/20 prompts with n=64 completions each\r\n",
      "Processed prompts: 100%|█| 256/256 [02:25<00:00,  1.76it/s, est. speed input: 15\r\n",
      "[Rank 0] Processed 20/20 prompts with n=64 completions each\r\n",
      "[Rank 0] Saved partial JSONL to outputs/amc23_instruct_t1.2.rank0.jsonl\r\n",
      "[rank0]:[W1202 21:20:35.644646707 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Processed prompts: 100%|█| 256/256 [02:37<00:00,  1.62it/s, est. speed input: 14\r\n",
      "[Rank 1] Processed 20/20 prompts with n=64 completions each\r\n",
      "[Rank 1] Saved partial JSONL to outputs/amc23_instruct_t1.2.rank1.jsonl\r\n",
      "[rank0]:[W1202 21:21:37.301590752 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Total time consuming is: 917.00s\r\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/amc23_instruct_t1.2.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B-Instruct\" \\\n",
    "  --dataset \"amc\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 64 \\\n",
    "  --temperature 1.2 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/amc23_instruct_t1.2.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a693b8a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T21:21:42.672132Z",
     "iopub.status.busy": "2025-12-02T21:21:42.671809Z",
     "iopub.status.idle": "2025-12-02T21:21:49.524546Z",
     "shell.execute_reply": "2025-12-02T21:21:49.523546Z"
    },
    "papermill": {
     "duration": 7.070033,
     "end_time": "2025-12-02T21:21:49.525944",
     "exception": false,
     "start_time": "2025-12-02T21:21:42.455911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/amc23_instruct_t1.2.jsonl...\r\n",
      "Scoring generations from outputs/amc23_instruct_t1.2.jsonl...\r\n",
      "Processing lines: 100%|████████████████████| 2560/2560 [00:05<00:00, 436.13it/s]\r\n",
      "Processing complete. Scored 2560 lines across 40 unique problems.\r\n",
      "\r\n",
      "Pass@k Metrics:\r\n",
      "  pass@1   : 46.91%\r\n",
      "  pass@2   : 57.60%\r\n",
      "  pass@4   : 66.72%\r\n",
      "  pass@8   : 74.20%\r\n",
      "  pass@16  : 80.07%\r\n",
      "  pass@32  : 83.78%\r\n",
      "  pass@64  : 85.00%\r\n",
      "\r\n",
      "Majority Vote Metric:\r\n",
      "  maj@1    : 57.50%\r\n",
      "\r\n",
      "Scored results saved to outputs/amc23_instruct_t1.2_eval.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py \\\n",
    "  --input_file outputs/amc23_instruct_t1.2.jsonl \\\n",
    "  --output_file outputs/amc23_instruct_t1.2_eval.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5740d429",
   "metadata": {
    "papermill": {
     "duration": 0.219792,
     "end_time": "2025-12-02T21:21:50.047700",
     "exception": false,
     "start_time": "2025-12-02T21:21:49.827908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### AIME25 Dataset (rollout-n = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d4e15a",
   "metadata": {
    "papermill": {
     "duration": 0.216226,
     "end_time": "2025-12-02T21:21:50.482965",
     "exception": false,
     "start_time": "2025-12-02T21:21:50.266739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Temperature = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56f36e0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T21:21:50.942111Z",
     "iopub.status.busy": "2025-12-02T21:21:50.941835Z",
     "iopub.status.idle": "2025-12-02T21:34:40.992979Z",
     "shell.execute_reply": "2025-12-02T21:34:40.992093Z"
    },
    "papermill": {
     "duration": 770.279258,
     "end_time": "2025-12-02T21:34:40.994554",
     "exception": false,
     "start_time": "2025-12-02T21:21:50.715296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 21:21:54.859428: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764710514.879695    2714 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764710514.885733    2714 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 21:22:01 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\r\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\r\n",
      "2025-12-02 21:22:08.885279: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-12-02 21:22:08.886162: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764710528.908135    2732 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764710528.908558    2733 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764710528.914811    2732 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "E0000 00:00:1764710528.915320    2733 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 21:22:15 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "INFO 12-02 21:22:15 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "[Rank 0] Logical GPU count: 1\r\n",
      "[Rank 0] Loading dataset: aime -> math-ai/aime25\r\n",
      "[Rank 1] Logical GPU count: 1\r\n",
      "[Rank 1] Loading dataset: aime -> math-ai/aime25\r\n",
      "[Rank 0] Needs to process 15 prompts (indices [0, 15))\r\n",
      "[Rank 1] Needs to process 15 prompts (indices [15, 30))\r\n",
      "WARNING 12-02 21:22:25 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 12-02 21:22:25 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-02 21:22:50 [config.py:717] This model supports multiple tasks: {'embed', 'classify', 'score', 'generate', 'reward'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 21:22:50 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 21:22:50 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 21:22:50 [config.py:717] This model supports multiple tasks: {'classify', 'generate', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 21:22:50 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 21:22:50 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 21:22:52 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 21:22:52 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 12-02 21:22:52 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 21:22:52 [cuda.py:289] Using XFormers backend.\r\n",
      "[W1202 21:22:52.691666465 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 21:22:52.692372837 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 21:22:52 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 21:22:52 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\r\n",
      "[W1202 21:22:53.986178115 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 21:22:53.986930501 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 21:22:53 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 21:22:53 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 21:22:53 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\r\n",
      "INFO 12-02 21:22:53 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "INFO 12-02 21:22:53 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 21:22:53 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.26s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.26s/it]\r\n",
      "\r\n",
      "INFO 12-02 21:22:56 [loader.py:458] Loading weights took 3.34 seconds\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.37s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.37s/it]\r\n",
      "\r\n",
      "INFO 12-02 21:22:56 [loader.py:458] Loading weights took 3.45 seconds\r\n",
      "INFO 12-02 21:22:57 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.792627 seconds\r\n",
      "INFO 12-02 21:22:57 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.872176 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 21:22:58 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 21:22:59 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 21:22:59 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 21:22:59 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 21:22:59 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 21:22:59 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 21:23:03 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 12-02 21:23:03 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.04it/s]\r\n",
      "INFO 12-02 21:23:37 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.19 GiB\r\n",
      "INFO 12-02 21:23:37 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 40.03 seconds\r\n",
      "[Rank 0] Starting generation for 15 prompts with batch_size=16\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.03it/s]\r\n",
      "INFO 12-02 21:23:37 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.19 GiB\r\n",
      "INFO 12-02 21:23:37 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 40.36 seconds\r\n",
      "[Rank 1] Starting generation for 15 prompts with batch_size=16\r\n",
      "Processed prompts: 100%|█| 960/960 [10:25<00:00,  1.54it/s, est. speed input: 30\r\n",
      "[Rank 1] Processed 15/15 prompts with n=64 completions each\r\n",
      "[Rank 1] Saved partial JSONL to outputs/aime25_instruct_t0.6.rank1.jsonl\r\n",
      "[rank0]:[W1202 21:34:05.857656671 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Processed prompts: 100%|█| 960/960 [10:56<00:00,  1.46it/s, est. speed input: 29\r\n",
      "[Rank 0] Processed 15/15 prompts with n=64 completions each\r\n",
      "[Rank 0] Saved partial JSONL to outputs/aime25_instruct_t0.6.rank0.jsonl\r\n",
      "[rank0]:[W1202 21:34:36.160941610 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Total time consuming is: 753.97s\r\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/aime25_instruct_t0.6.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B-Instruct\" \\\n",
    "  --dataset \"aime\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 64 \\\n",
    "  --temperature 0.6 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/aime25_instruct_t0.6.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47c7ffa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T21:34:41.455856Z",
     "iopub.status.busy": "2025-12-02T21:34:41.455574Z",
     "iopub.status.idle": "2025-12-02T21:34:50.460395Z",
     "shell.execute_reply": "2025-12-02T21:34:50.459504Z"
    },
    "papermill": {
     "duration": 9.234464,
     "end_time": "2025-12-02T21:34:50.461869",
     "exception": false,
     "start_time": "2025-12-02T21:34:41.227405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/aime25_instruct_t0.6.jsonl...\r\n",
      "Scoring generations from outputs/aime25_instruct_t0.6.jsonl...\r\n",
      "Processing lines:  84%|████████████████▋   | 1606/1920 [00:04<00:00, 337.49it/s]Timeout during comparison\r\n",
      "Processing lines:  85%|█████████████████▉   | 1640/1920 [00:06<00:03, 81.45it/s]Timeout during comparison\r\n",
      "Processing lines: 100%|████████████████████| 1920/1920 [00:08<00:00, 238.43it/s]\r\n",
      "Processing complete. Scored 1920 lines across 30 unique problems.\r\n",
      "\r\n",
      "Pass@k Metrics:\r\n",
      "  pass@1   : 9.22%\r\n",
      "  pass@2   : 14.78%\r\n",
      "  pass@4   : 20.98%\r\n",
      "  pass@8   : 26.92%\r\n",
      "  pass@16  : 32.57%\r\n",
      "  pass@32  : 37.71%\r\n",
      "  pass@64  : 43.33%\r\n",
      "\r\n",
      "Majority Vote Metric:\r\n",
      "  maj@1    : 13.33%\r\n",
      "\r\n",
      "Scored results saved to outputs/aime25_instruct_t0.6_eval.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py \\\n",
    "  --input_file outputs/aime25_instruct_t0.6.jsonl \\\n",
    "  --output_file outputs/aime25_instruct_t0.6_eval.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6954a3f",
   "metadata": {
    "papermill": {
     "duration": 0.226546,
     "end_time": "2025-12-02T21:34:50.917382",
     "exception": false,
     "start_time": "2025-12-02T21:34:50.690836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "679a920d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T21:34:51.371149Z",
     "iopub.status.busy": "2025-12-02T21:34:51.370609Z",
     "iopub.status.idle": "2025-12-02T21:47:06.373756Z",
     "shell.execute_reply": "2025-12-02T21:47:06.372808Z"
    },
    "papermill": {
     "duration": 735.234744,
     "end_time": "2025-12-02T21:47:06.375276",
     "exception": false,
     "start_time": "2025-12-02T21:34:51.140532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 21:34:55.253432: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764711295.274736    2885 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764711295.281204    2885 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 21:35:01 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\r\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\r\n",
      "2025-12-02 21:35:09.252963: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-12-02 21:35:09.256675: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764711309.274237    2904 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764711309.278073    2903 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764711309.280693    2904 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "E0000 00:00:1764711309.285656    2903 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 21:35:15 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "INFO 12-02 21:35:15 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "[Rank 0] Logical GPU count: 1\r\n",
      "[Rank 0] Loading dataset: aime -> math-ai/aime25\r\n",
      "[Rank 1] Logical GPU count: 1\r\n",
      "[Rank 1] Loading dataset: aime -> math-ai/aime25\r\n",
      "[Rank 0] Needs to process 15 prompts (indices [0, 15))\r\n",
      "[Rank 1] Needs to process 15 prompts (indices [15, 30))\r\n",
      "WARNING 12-02 21:35:25 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 12-02 21:35:25 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-02 21:35:50 [config.py:717] This model supports multiple tasks: {'generate', 'classify', 'embed', 'score', 'reward'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 21:35:50 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 21:35:50 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 21:35:50 [config.py:717] This model supports multiple tasks: {'score', 'classify', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 21:35:50 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 21:35:50 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 21:35:52 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 21:35:52 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 12-02 21:35:52 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 21:35:52 [cuda.py:289] Using XFormers backend.\r\n",
      "[W1202 21:35:52.744394278 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 21:35:52.745084572 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 21:35:52 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 21:35:52 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\r\n",
      "[W1202 21:35:52.798621986 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 21:35:52.799330468 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 21:35:52 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 21:35:52 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\r\n",
      "INFO 12-02 21:35:53 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 21:35:53 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 21:35:53 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "INFO 12-02 21:35:53 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.30s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.30s/it]\r\n",
      "\r\n",
      "INFO 12-02 21:35:56 [loader.py:458] Loading weights took 3.38 seconds\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.34s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.34s/it]\r\n",
      "\r\n",
      "INFO 12-02 21:35:56 [loader.py:458] Loading weights took 3.45 seconds\r\n",
      "INFO 12-02 21:35:57 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.820076 seconds\r\n",
      "INFO 12-02 21:35:57 [model_runner.py:1140] Model loading took 2.8798 GiB and 4.027474 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 21:35:58 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 21:35:59 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 21:35:59 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 21:35:59 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 21:35:59 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 21:35:59 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 21:36:03 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 12-02 21:36:04 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.03it/s]\r\n",
      "INFO 12-02 21:36:37 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.19 GiB\r\n",
      "INFO 12-02 21:36:37 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 40.23 seconds\r\n",
      "[Rank 0] Starting generation for 15 prompts with batch_size=16\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.03it/s]\r\n",
      "INFO 12-02 21:36:37 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.19 GiB\r\n",
      "INFO 12-02 21:36:37 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 40.44 seconds\r\n",
      "[Rank 1] Starting generation for 15 prompts with batch_size=16\r\n",
      "Processed prompts: 100%|█| 960/960 [09:33<00:00,  1.67it/s, est. speed input: 33\r\n",
      "[Rank 1] Processed 15/15 prompts with n=64 completions each\r\n",
      "[Rank 1] Saved partial JSONL to outputs/aime25_instruct_t1.0.rank1.jsonl\r\n",
      "[rank0]:[W1202 21:46:14.103410062 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Processed prompts: 100%|█| 960/960 [10:21<00:00,  1.54it/s, est. speed input: 31\r\n",
      "[Rank 0] Processed 15/15 prompts with n=64 completions each\r\n",
      "[Rank 0] Saved partial JSONL to outputs/aime25_instruct_t1.0.rank0.jsonl\r\n",
      "[rank0]:[W1202 21:47:01.591226999 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Total time consuming is: 718.97s\r\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/aime25_instruct_t1.0.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B-Instruct\" \\\n",
    "  --dataset \"aime\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 64 \\\n",
    "  --temperature 1.0 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/aime25_instruct_t1.0.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "539ab90f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T21:47:06.841633Z",
     "iopub.status.busy": "2025-12-02T21:47:06.840911Z",
     "iopub.status.idle": "2025-12-02T21:47:16.921980Z",
     "shell.execute_reply": "2025-12-02T21:47:16.921253Z"
    },
    "papermill": {
     "duration": 10.312467,
     "end_time": "2025-12-02T21:47:16.923356",
     "exception": false,
     "start_time": "2025-12-02T21:47:06.610889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/aime25_instruct_t1.0.jsonl...\r\n",
      "Scoring generations from outputs/aime25_instruct_t1.0.jsonl...\r\n",
      "Processing lines:  84%|████████████████▊   | 1615/1920 [00:05<00:00, 337.49it/s]Timeout during comparison\r\n",
      "Timeout during comparison\r\n",
      "Timeout during comparison\r\n",
      "Processing lines: 100%|████████████████████| 1920/1920 [00:09<00:00, 210.79it/s]\r\n",
      "Processing complete. Scored 1920 lines across 30 unique problems.\r\n",
      "\r\n",
      "Pass@k Metrics:\r\n",
      "  pass@1   : 8.65%\r\n",
      "  pass@2   : 14.55%\r\n",
      "  pass@4   : 21.98%\r\n",
      "  pass@8   : 29.66%\r\n",
      "  pass@16  : 36.93%\r\n",
      "  pass@32  : 44.55%\r\n",
      "  pass@64  : 53.33%\r\n",
      "\r\n",
      "Majority Vote Metric:\r\n",
      "  maj@1    : 16.67%\r\n",
      "\r\n",
      "Scored results saved to outputs/aime25_instruct_t1.0_eval.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py \\\n",
    "  --input_file outputs/aime25_instruct_t1.0.jsonl \\\n",
    "  --output_file outputs/aime25_instruct_t1.0_eval.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621de6f7",
   "metadata": {
    "papermill": {
     "duration": 0.234313,
     "end_time": "2025-12-02T21:47:17.392837",
     "exception": false,
     "start_time": "2025-12-02T21:47:17.158524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Temperature = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "395f8c43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T21:47:17.866919Z",
     "iopub.status.busy": "2025-12-02T21:47:17.866638Z",
     "iopub.status.idle": "2025-12-02T22:01:44.350232Z",
     "shell.execute_reply": "2025-12-02T22:01:44.349512Z"
    },
    "papermill": {
     "duration": 866.725435,
     "end_time": "2025-12-02T22:01:44.351701",
     "exception": false,
     "start_time": "2025-12-02T21:47:17.626266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 21:47:21.751223: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764712041.771802    3056 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764712041.777913    3056 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 21:47:27 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "DP Rank 0 (Local 0) mapped to GPU(s): 0\r\n",
      "DP Rank 1 (Local 1) mapped to GPU(s): 1\r\n",
      "2025-12-02 21:47:35.710604: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-12-02 21:47:35.728395: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764712055.733587    3074 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764712055.739711    3074 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1764712055.748253    3075 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1764712055.754432    3075 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 12-02 21:47:42 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "INFO 12-02 21:47:42 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "[Rank 1] Logical GPU count: 1\r\n",
      "[Rank 1] Loading dataset: aime -> math-ai/aime25\r\n",
      "[Rank 0] Logical GPU count: 1\r\n",
      "[Rank 0] Loading dataset: aime -> math-ai/aime25\r\n",
      "[Rank 1] Needs to process 15 prompts (indices [15, 30))\r\n",
      "[Rank 0] Needs to process 15 prompts (indices [0, 15))\r\n",
      "WARNING 12-02 21:47:52 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 12-02 21:47:52 [config.py:2972] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 12-02 21:48:16 [config.py:717] This model supports multiple tasks: {'classify', 'generate', 'embed', 'score', 'reward'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 21:48:16 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 21:48:16 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 21:48:16 [config.py:717] This model supports multiple tasks: {'score', 'classify', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.\r\n",
      "WARNING 12-02 21:48:16 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "INFO 12-02 21:48:16 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \r\n",
      "INFO 12-02 21:48:18 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 21:48:18 [cuda.py:289] Using XFormers backend.\r\n",
      "INFO 12-02 21:48:18 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 12-02 21:48:18 [cuda.py:289] Using XFormers backend.\r\n",
      "[W1202 21:48:19.063366016 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 21:48:19.064132111 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 21:48:19.068971864 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1202 21:48:19.069582100 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 12-02 21:48:19 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 21:48:19 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\r\n",
      "INFO 12-02 21:48:19 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 12-02 21:48:19 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-Math-1.5B-Instruct...\r\n",
      "INFO 12-02 21:48:19 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 21:48:19 [weight_utils.py:265] Using model weights format ['*.safetensors']\r\n",
      "INFO 12-02 21:48:19 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "INFO 12-02 21:48:19 [weight_utils.py:315] No model.safetensors.index.json found in remote.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.41s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.41s/it]\r\n",
      "\r\n",
      "INFO 12-02 21:48:23 [loader.py:458] Loading weights took 3.49 seconds\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.46s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.46s/it]\r\n",
      "\r\n",
      "INFO 12-02 21:48:23 [loader.py:458] Loading weights took 3.55 seconds\r\n",
      "INFO 12-02 21:48:23 [model_runner.py:1140] Model loading took 2.8798 GiB and 3.914757 seconds\r\n",
      "INFO 12-02 21:48:23 [model_runner.py:1140] Model loading took 2.8798 GiB and 4.140732 seconds\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 21:48:25 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 12-02 21:48:25 [worker.py:287] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 8.95GiB.\r\n",
      "INFO 12-02 21:48:25 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 21:48:25 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 21:48:25 [executor_base.py:112] # cuda blocks: 20939, # CPU blocks: 9362\r\n",
      "INFO 12-02 21:48:25 [executor_base.py:117] Maximum concurrency for 3072 tokens per request: 109.06x\r\n",
      "INFO 12-02 21:48:30 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes:   0%|                       | 0/35 [00:00<?, ?it/s]INFO 12-02 21:48:30 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.05it/s]\r\n",
      "INFO 12-02 21:49:03 [model_runner.py:1592] Graph capturing finished in 33 secs, took 0.19 GiB\r\n",
      "INFO 12-02 21:49:03 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 40.06 seconds\r\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:33<00:00,  1.04it/s]\r\n",
      "INFO 12-02 21:49:03 [model_runner.py:1592] Graph capturing finished in 34 secs, took 0.19 GiB\r\n",
      "INFO 12-02 21:49:03 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 40.19 seconds\r\n",
      "[Rank 0] Starting generation for 15 prompts with batch_size=16\r\n",
      "[Rank 1] Starting generation for 15 prompts with batch_size=16\r\n",
      "Processed prompts: 100%|█| 960/960 [12:06<00:00,  1.32it/s, est. speed input: 26\r\n",
      "[Rank 0] Processed 15/15 prompts with n=64 completions each\r\n",
      "[Rank 0] Saved partial JSONL to outputs/aime25_instruct_t1.2.rank0.jsonl\r\n",
      "Processed prompts:  80%|▊| 768/960 [12:08<01:27,  2.19it/s, est. speed input: 23[rank0]:[W1202 22:01:13.196689001 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Processed prompts: 100%|█| 960/960 [12:32<00:00,  1.28it/s, est. speed input: 25\r\n",
      "[Rank 1] Processed 15/15 prompts with n=64 completions each\r\n",
      "[Rank 1] Saved partial JSONL to outputs/aime25_instruct_t1.2.rank1.jsonl\r\n",
      "[rank0]:[W1202 22:01:39.514388017 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Total time consuming is: 850.43s\r\n",
      "Merged 2 partial files into /kaggle/working/src/outputs/aime25_instruct_t1.2.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py \\\n",
    "  --model \"Qwen/Qwen2.5-Math-1.5B-Instruct\" \\\n",
    "  --dataset \"aime\" \\\n",
    "  --dp-size 2 \\\n",
    "  --batch-size 16 \\\n",
    "  --rollout-n 64 \\\n",
    "  --temperature 1.2 \\\n",
    "  --top-p 0.9 \\\n",
    "  --output_file outputs/aime25_instruct_t1.2.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2f55cff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T22:01:44.915950Z",
     "iopub.status.busy": "2025-12-02T22:01:44.915310Z",
     "iopub.status.idle": "2025-12-02T22:01:52.624652Z",
     "shell.execute_reply": "2025-12-02T22:01:52.623918Z"
    },
    "papermill": {
     "duration": 8.033231,
     "end_time": "2025-12-02T22:01:52.626647",
     "exception": false,
     "start_time": "2025-12-02T22:01:44.593416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in outputs/aime25_instruct_t1.2.jsonl...\r\n",
      "Scoring generations from outputs/aime25_instruct_t1.2.jsonl...\r\n",
      "Processing lines:  84%|████████████████▊   | 1618/1920 [00:04<00:00, 388.87it/s]Timeout during comparison\r\n",
      "Processing lines: 100%|████████████████████| 1920/1920 [00:06<00:00, 285.60it/s]\r\n",
      "Processing complete. Scored 1920 lines across 30 unique problems.\r\n",
      "\r\n",
      "Pass@k Metrics:\r\n",
      "  pass@1   : 8.23%\r\n",
      "  pass@2   : 13.45%\r\n",
      "  pass@4   : 19.53%\r\n",
      "  pass@8   : 25.40%\r\n",
      "  pass@16  : 30.51%\r\n",
      "  pass@32  : 34.77%\r\n",
      "  pass@64  : 40.00%\r\n",
      "\r\n",
      "Majority Vote Metric:\r\n",
      "  maj@1    : 20.00%\r\n",
      "\r\n",
      "Scored results saved to outputs/aime25_instruct_t1.2_eval.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py \\\n",
    "  --input_file outputs/aime25_instruct_t1.2.jsonl \\\n",
    "  --output_file outputs/aime25_instruct_t1.2_eval.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a67a6e",
   "metadata": {
    "papermill": {
     "duration": 0.246855,
     "end_time": "2025-12-02T22:01:53.121768",
     "exception": false,
     "start_time": "2025-12-02T22:01:52.874913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Task 2: Analyze and Compare Results\n",
    "\n",
    "Extract metrics from evaluation results and create summary tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "daa47ac2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T22:01:53.599177Z",
     "iopub.status.busy": "2025-12-02T22:01:53.598578Z",
     "iopub.status.idle": "2025-12-02T22:01:53.877708Z",
     "shell.execute_reply": "2025-12-02T22:01:53.876847Z"
    },
    "papermill": {
     "duration": 0.517674,
     "end_time": "2025-12-02T22:01:53.878930",
     "exception": false,
     "start_time": "2025-12-02T22:01:53.361256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION CONFIGURATIONS\n",
      "================================================================================\n",
      "Base_Math500_0.6               -> math500_base_t0.6_eval.jsonl\n",
      "Base_Math500_1.0               -> math500_base_t1.0_eval.jsonl\n",
      "Base_Math500_1.2               -> math500_base_t1.2_eval.jsonl\n",
      "Base_AMC23_0.6                 -> amc23_base_t0.6_eval.jsonl\n",
      "Base_AMC23_1.0                 -> amc23_base_t1.0_eval.jsonl\n",
      "Base_AMC23_1.2                 -> amc23_base_t1.2_eval.jsonl\n",
      "Base_AIME25_0.6                -> aime25_base_t0.6_eval.jsonl\n",
      "Base_AIME25_1.0                -> aime25_base_t1.0_eval.jsonl\n",
      "Base_AIME25_1.2                -> aime25_base_t1.2_eval.jsonl\n",
      "Instruct_Math500_0.6           -> math500_instruct_t0.6_eval.jsonl\n",
      "Instruct_Math500_1.0           -> math500_instruct_t1.0_eval.jsonl\n",
      "Instruct_Math500_1.2           -> math500_instruct_t1.2_eval.jsonl\n",
      "Instruct_AMC23_0.6             -> amc23_instruct_t0.6_eval.jsonl\n",
      "Instruct_AMC23_1.0             -> amc23_instruct_t1.0_eval.jsonl\n",
      "Instruct_AMC23_1.2             -> amc23_instruct_t1.2_eval.jsonl\n",
      "Instruct_AIME25_0.6            -> aime25_instruct_t0.6_eval.jsonl\n",
      "Instruct_AIME25_1.0            -> aime25_instruct_t1.0_eval.jsonl\n",
      "Instruct_AIME25_1.2            -> aime25_instruct_t1.2_eval.jsonl\n",
      "\n",
      "Note: Run the generation and evaluation cells above to generate these files.\n",
      "Once complete, extract metrics from the output JSONL files for analysis.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Define evaluation files for all configurations\n",
    "eval_configs = {\n",
    "    \"Base_Math500_0.6\": \"math500_base_t0.6_eval.jsonl\",\n",
    "    \"Base_Math500_1.0\": \"math500_base_t1.0_eval.jsonl\",\n",
    "    \"Base_Math500_1.2\": \"math500_base_t1.2_eval.jsonl\",\n",
    "    \"Base_AMC23_0.6\": \"amc23_base_t0.6_eval.jsonl\",\n",
    "    \"Base_AMC23_1.0\": \"amc23_base_t1.0_eval.jsonl\",\n",
    "    \"Base_AMC23_1.2\": \"amc23_base_t1.2_eval.jsonl\",\n",
    "    \"Base_AIME25_0.6\": \"aime25_base_t0.6_eval.jsonl\",\n",
    "    \"Base_AIME25_1.0\": \"aime25_base_t1.0_eval.jsonl\",\n",
    "    \"Base_AIME25_1.2\": \"aime25_base_t1.2_eval.jsonl\",\n",
    "    \"Instruct_Math500_0.6\": \"math500_instruct_t0.6_eval.jsonl\",\n",
    "    \"Instruct_Math500_1.0\": \"math500_instruct_t1.0_eval.jsonl\",\n",
    "    \"Instruct_Math500_1.2\": \"math500_instruct_t1.2_eval.jsonl\",\n",
    "    \"Instruct_AMC23_0.6\": \"amc23_instruct_t0.6_eval.jsonl\",\n",
    "    \"Instruct_AMC23_1.0\": \"amc23_instruct_t1.0_eval.jsonl\",\n",
    "    \"Instruct_AMC23_1.2\": \"amc23_instruct_t1.2_eval.jsonl\",\n",
    "    \"Instruct_AIME25_0.6\": \"aime25_instruct_t0.6_eval.jsonl\",\n",
    "    \"Instruct_AIME25_1.0\": \"aime25_instruct_t1.0_eval.jsonl\",\n",
    "    \"Instruct_AIME25_1.2\": \"aime25_instruct_t1.2_eval.jsonl\",\n",
    "}\n",
    "\n",
    "def extract_metrics_from_eval_file(filepath):\n",
    "    \"\"\"\n",
    "    Extract pass@k and maj@1 metrics from evaluation output.\n",
    "    Returns a dict with metrics.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    if Path(filepath).exists():\n",
    "        # Read the eval file to find pass@k and maj@1 metrics\n",
    "        # These would be printed during evaluation and saved\n",
    "        metrics['file_exists'] = True\n",
    "    else:\n",
    "        metrics['file_exists'] = False\n",
    "    return metrics\n",
    "\n",
    "# Display summary\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION CONFIGURATIONS\")\n",
    "print(\"=\" * 80)\n",
    "for config_name, filepath in eval_configs.items():\n",
    "    print(f\"{config_name:30} -> {filepath}\")\n",
    "print(\"\\nNote: Run the generation and evaluation cells above to generate these files.\")\n",
    "print(\"Once complete, extract metrics from the output JSONL files for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a2d00",
   "metadata": {},
   "source": [
    "## Task 3: Analyze and Visualize Results\n",
    "\n",
    "Now we will run the analysis script to process all evaluation outputs, generate summary files (`all_results.json`, `summary.csv`), and create plots in the `outputs/plots/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdab3234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 1: Evaluating generation files ---\n",
      "Found 18 raw generation files to process.\n",
      "Scoring aime25_base_t0.6.jsonl:  81%|████▊ | 1552/1920 [00:03<00:00, 506.12it/s]Timeout during comparison\n",
      "Scoring aime25_base_t0.6.jsonl: 100%|██████| 1920/1920 [00:04<00:00, 403.90it/s]\n",
      "\n",
      "--- Metrics for aime25_base_t0.6.jsonl ---\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 4.22%\n",
      "  pass@2   : 7.52%\n",
      "  pass@4   : 12.32%\n",
      "  pass@8   : 18.13%\n",
      "  pass@16  : 23.96%\n",
      "  pass@32  : 29.83%\n",
      "  pass@64  : 36.67%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 3.33%\n",
      "Scored results saved to outputs/aime25_base_t0.6_eval.jsonl\n",
      "\n",
      "Scoring aime25_base_t1.0.jsonl: 100%|██████| 1920/1920 [00:04<00:00, 388.15it/s]\n",
      "\n",
      "--- Metrics for aime25_base_t1.0.jsonl ---\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 2.76%\n",
      "  pass@2   : 5.09%\n",
      "  pass@4   : 8.74%\n",
      "  pass@8   : 13.53%\n",
      "  pass@16  : 18.72%\n",
      "  pass@32  : 24.58%\n",
      "  pass@64  : 33.33%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 3.33%\n",
      "Scored results saved to outputs/aime25_base_t1.0_eval.jsonl\n",
      "\n",
      "Scoring aime25_base_t1.2.jsonl: 100%|██████| 1920/1920 [00:03<00:00, 520.20it/s]\n",
      "\n",
      "--- Metrics for aime25_base_t1.2.jsonl ---\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 1.25%\n",
      "  pass@2   : 2.43%\n",
      "  pass@4   : 4.62%\n",
      "  pass@8   : 8.38%\n",
      "  pass@16  : 14.10%\n",
      "  pass@32  : 21.66%\n",
      "  pass@64  : 30.00%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 0.00%\n",
      "Scored results saved to outputs/aime25_base_t1.2_eval.jsonl\n",
      "\n",
      "Scoring aime25_instruct_t0.6.jsonl:  84%|█▋| 1621/1920 [00:03<00:00, 511.39it/s]Timeout during comparison\n",
      "Timeout during comparison\n",
      "Scoring aime25_instruct_t0.6.jsonl: 100%|██| 1920/1920 [00:05<00:00, 334.05it/s]\n",
      "\n",
      "--- Metrics for aime25_instruct_t0.6.jsonl ---\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 9.22%\n",
      "  pass@2   : 14.78%\n",
      "  pass@4   : 20.98%\n",
      "  pass@8   : 26.92%\n",
      "  pass@16  : 32.57%\n",
      "  pass@32  : 37.71%\n",
      "  pass@64  : 43.33%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 13.33%\n",
      "Scored results saved to outputs/aime25_instruct_t0.6_eval.jsonl\n",
      "\n",
      "Scoring aime25_instruct_t1.0.jsonl:  83%|█▋| 1587/1920 [00:03<00:00, 519.34it/s]Timeout during comparison\n",
      "Timeout during comparison\n",
      "Scoring aime25_instruct_t1.0.jsonl:  85%|██▌| 1640/1920 [00:05<00:03, 70.51it/s]Timeout during comparison\n",
      "Scoring aime25_instruct_t1.0.jsonl: 100%|██| 1920/1920 [00:06<00:00, 279.30it/s]\n",
      "\n",
      "--- Metrics for aime25_instruct_t1.0.jsonl ---\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 8.65%\n",
      "  pass@2   : 14.55%\n",
      "  pass@4   : 21.98%\n",
      "  pass@8   : 29.66%\n",
      "  pass@16  : 36.93%\n",
      "  pass@32  : 44.55%\n",
      "  pass@64  : 53.33%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 16.67%\n",
      "Scored results saved to outputs/aime25_instruct_t1.0_eval.jsonl\n",
      "\n",
      "Scoring aime25_instruct_t1.2.jsonl:  86%|█▋| 1642/1920 [00:02<00:00, 584.41it/s]Timeout during comparison\n",
      "Scoring aime25_instruct_t1.2.jsonl: 100%|██| 1920/1920 [00:04<00:00, 447.12it/s]\n",
      "\n",
      "--- Metrics for aime25_instruct_t1.2.jsonl ---\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 8.23%\n",
      "  pass@2   : 13.45%\n",
      "  pass@4   : 19.53%\n",
      "  pass@8   : 25.40%\n",
      "  pass@16  : 30.51%\n",
      "  pass@32  : 34.77%\n",
      "  pass@64  : 40.00%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 20.00%\n",
      "Scored results saved to outputs/aime25_instruct_t1.2_eval.jsonl\n",
      "\n",
      "Scoring amc23_base_t0.6.jsonl: 100%|███████| 2560/2560 [00:03<00:00, 721.74it/s]\n",
      "\n",
      "--- Metrics for amc23_base_t0.6.jsonl ---\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 34.45%\n",
      "  pass@2   : 48.57%\n",
      "  pass@4   : 61.68%\n",
      "  pass@8   : 72.23%\n",
      "  pass@16  : 80.21%\n",
      "  pass@32  : 86.84%\n",
      "  pass@64  : 92.50%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 47.50%\n",
      "Scored results saved to outputs/amc23_base_t0.6_eval.jsonl\n",
      "\n",
      "Scoring amc23_base_t1.0.jsonl: 100%|███████| 2560/2560 [00:05<00:00, 475.53it/s]\n",
      "\n",
      "--- Metrics for amc23_base_t1.0.jsonl ---\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 25.70%\n",
      "  pass@2   : 40.05%\n",
      "  pass@4   : 55.36%\n",
      "  pass@8   : 68.97%\n",
      "  pass@16  : 79.66%\n",
      "  pass@32  : 87.80%\n",
      "  pass@64  : 95.00%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 40.00%\n",
      "Scored results saved to outputs/amc23_base_t1.0_eval.jsonl\n",
      "\n",
      "Scoring amc23_base_t1.2.jsonl:   5%|▎       | 116/2560 [00:00<00:04, 510.11it/s]Timeout during comparison\n",
      "Scoring amc23_base_t1.2.jsonl: 100%|███████| 2560/2560 [00:07<00:00, 354.13it/s]\n",
      "\n",
      "--- Metrics for amc23_base_t1.2.jsonl ---\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 14.30%\n",
      "  pass@2   : 24.84%\n",
      "  pass@4   : 39.23%\n",
      "  pass@8   : 55.01%\n",
      "  pass@16  : 68.90%\n",
      "  pass@32  : 79.57%\n",
      "  pass@64  : 87.50%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 7.50%\n",
      "Scored results saved to outputs/amc23_base_t1.2_eval.jsonl\n",
      "\n",
      "Scoring amc23_instruct_t0.6.jsonl: 100%|███| 2560/2560 [00:03<00:00, 686.63it/s]\n",
      "\n",
      "--- Metrics for amc23_instruct_t0.6.jsonl ---\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 53.36%\n",
      "  pass@2   : 63.30%\n",
      "  pass@4   : 70.77%\n",
      "  pass@8   : 76.37%\n",
      "  pass@16  : 81.60%\n",
      "  pass@32  : 86.64%\n",
      "  pass@64  : 92.50%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 67.50%\n",
      "Scored results saved to outputs/amc23_instruct_t0.6_eval.jsonl\n",
      "\n",
      "Scoring amc23_instruct_t1.0.jsonl: 100%|███| 2560/2560 [00:03<00:00, 705.80it/s]\n",
      "\n",
      "--- Metrics for amc23_instruct_t1.0.jsonl ---\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 52.27%\n",
      "  pass@2   : 63.47%\n",
      "  pass@4   : 72.55%\n",
      "  pass@8   : 79.53%\n",
      "  pass@16  : 85.08%\n",
      "  pass@32  : 88.71%\n",
      "  pass@64  : 90.00%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 65.00%\n",
      "Scored results saved to outputs/amc23_instruct_t1.0_eval.jsonl\n",
      "\n",
      "Scoring amc23_instruct_t1.2.jsonl: 100%|███| 2560/2560 [00:03<00:00, 760.31it/s]\n",
      "\n",
      "--- Metrics for amc23_instruct_t1.2.jsonl ---\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 46.91%\n",
      "  pass@2   : 57.60%\n",
      "  pass@4   : 66.72%\n",
      "  pass@8   : 74.20%\n",
      "  pass@16  : 80.07%\n",
      "  pass@32  : 83.78%\n",
      "  pass@64  : 85.00%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 57.50%\n",
      "Scored results saved to outputs/amc23_instruct_t1.2_eval.jsonl\n",
      "\n",
      "Scoring math500_base_t0.6.jsonl: 100%|█████| 8000/8000 [00:17<00:00, 460.79it/s]\n",
      "\n",
      "--- Metrics for math500_base_t0.6.jsonl ---\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 35.99%\n",
      "  pass@2   : 51.28%\n",
      "  pass@4   : 65.74%\n",
      "  pass@8   : 77.06%\n",
      "  pass@16  : 84.60%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 45.20%\n",
      "Scored results saved to outputs/math500_base_t0.6_eval.jsonl\n",
      "\n",
      "Scoring math500_base_t1.0.jsonl:  52%|██▌  | 4162/8000 [00:13<00:10, 372.70it/s]Timeout during comparison\n",
      "Scoring math500_base_t1.0.jsonl: 100%|█████| 8000/8000 [00:28<00:00, 284.12it/s]\n",
      "\n",
      "--- Metrics for math500_base_t1.0.jsonl ---\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 28.85%\n",
      "  pass@2   : 44.83%\n",
      "  pass@4   : 61.93%\n",
      "  pass@8   : 76.61%\n",
      "  pass@16  : 86.00%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 40.00%\n",
      "Scored results saved to outputs/math500_base_t1.0_eval.jsonl\n",
      "\n",
      "Scoring math500_base_t1.2.jsonl:   8%|▍     | 634/8000 [00:01<00:17, 431.16it/s]Timeout during comparison\n",
      "Scoring math500_base_t1.2.jsonl:  50%|██▌  | 4036/8000 [00:11<00:14, 266.90it/s]Timeout during comparison\n",
      "Scoring math500_base_t1.2.jsonl: 100%|█████| 8000/8000 [00:23<00:00, 346.54it/s]\n",
      "\n",
      "--- Metrics for math500_base_t1.2.jsonl ---\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 18.80%\n",
      "  pass@2   : 31.79%\n",
      "  pass@4   : 48.68%\n",
      "  pass@8   : 66.21%\n",
      "  pass@16  : 80.00%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 16.20%\n",
      "Scored results saved to outputs/math500_base_t1.2_eval.jsonl\n",
      "\n",
      "Scoring math500_instruct_t0.6.jsonl: 100%|█| 8000/8000 [00:14<00:00, 567.69it/s]\n",
      "\n",
      "--- Metrics for math500_instruct_t0.6.jsonl ---\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 75.00%\n",
      "  pass@2   : 80.49%\n",
      "  pass@4   : 84.83%\n",
      "  pass@8   : 88.19%\n",
      "  pass@16  : 90.20%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 79.40%\n",
      "Scored results saved to outputs/math500_instruct_t0.6_eval.jsonl\n",
      "\n",
      "Scoring math500_instruct_t1.0.jsonl: 100%|█| 8000/8000 [00:14<00:00, 555.89it/s]\n",
      "\n",
      "--- Metrics for math500_instruct_t1.0.jsonl ---\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 74.42%\n",
      "  pass@2   : 80.90%\n",
      "  pass@4   : 85.77%\n",
      "  pass@8   : 89.32%\n",
      "  pass@16  : 92.20%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 80.20%\n",
      "Scored results saved to outputs/math500_instruct_t1.0_eval.jsonl\n",
      "\n",
      "Scoring math500_instruct_t1.2.jsonl: 100%|█| 8000/8000 [00:14<00:00, 564.28it/s]\n",
      "\n",
      "--- Metrics for math500_instruct_t1.2.jsonl ---\n",
      "Pass@k Metrics:\n",
      "  pass@1   : 70.20%\n",
      "  pass@2   : 77.66%\n",
      "  pass@4   : 83.00%\n",
      "  pass@8   : 87.04%\n",
      "  pass@16  : 90.20%\n",
      "\n",
      "Majority Vote Metric:\n",
      "  maj@1    : 76.20%\n",
      "Scored results saved to outputs/math500_instruct_t1.2_eval.jsonl\n",
      "\n",
      "\n",
      "--- Stage 2: Aggregating results and generating reports ---\n",
      "Saved detailed summary to: outputs/all_results.json\n",
      "Saved CSV summary to: outputs/summary.csv\n",
      "\n",
      "--- Generating Plots ---\n",
      "Saved plot: outputs/plots/plot_Base_AIME25_pass_k_vs_temp.png\n",
      "Saved plot: outputs/plots/plot_Base_AMC23_pass_k_vs_temp.png\n",
      "Saved plot: outputs/plots/plot_Base_Math500_pass_k_vs_temp.png\n",
      "Saved plot: outputs/plots/plot_Instruct_AIME25_pass_k_vs_temp.png\n",
      "Saved plot: outputs/plots/plot_Instruct_AMC23_pass_k_vs_temp.png\n",
      "Saved plot: outputs/plots/plot_Instruct_Math500_pass_k_vs_temp.png\n",
      "Saved plot: outputs/plots/plot_AIME25_temp_0.6_model_comparison.png\n",
      "Saved plot: outputs/plots/plot_AIME25_temp_1.0_model_comparison.png\n",
      "Saved plot: outputs/plots/plot_AIME25_temp_1.2_model_comparison.png\n",
      "Saved plot: outputs/plots/plot_AMC23_temp_0.6_model_comparison.png\n",
      "Saved plot: outputs/plots/plot_AMC23_temp_1.0_model_comparison.png\n",
      "Saved plot: outputs/plots/plot_AMC23_temp_1.2_model_comparison.png\n",
      "Saved plot: outputs/plots/plot_Math500_temp_0.6_model_comparison.png\n",
      "Saved plot: outputs/plots/plot_Math500_temp_1.0_model_comparison.png\n",
      "Saved plot: outputs/plots/plot_Math500_temp_1.2_model_comparison.png\n",
      "Saved plot: outputs/plots/plot_Base_temp_0.6_dataset_comparison.png\n",
      "Saved plot: outputs/plots/plot_Base_temp_1.0_dataset_comparison.png\n",
      "Saved plot: outputs/plots/plot_Base_temp_1.2_dataset_comparison.png\n",
      "Saved plot: outputs/plots/plot_Instruct_temp_0.6_dataset_comparison.png\n",
      "Saved plot: outputs/plots/plot_Instruct_temp_1.0_dataset_comparison.png\n",
      "Saved plot: outputs/plots/plot_Instruct_temp_1.2_dataset_comparison.png\n",
      "Saved plot: outputs/plots/heatmap_maj1.png\n",
      "Saved plot: outputs/plots/heatmap_pass1.png\n",
      "Saved plot: outputs/plots/heatmap_pass2.png\n",
      "Saved plot: outputs/plots/heatmap_pass4.png\n",
      "Saved plot: outputs/plots/heatmap_pass8.png\n",
      "Saved plot: outputs/plots/heatmap_pass16.png\n",
      "Saved plot: outputs/plots/heatmap_pass32.png\n",
      "Saved plot: outputs/plots/heatmap_pass64.png\n",
      "\n",
      "--- Analysis Complete ---\n"
     ]
    }
   ],
   "source": [
    "!python src/analysis.py --input_dir outputs --output_dir outputs"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7425745,
     "sourceId": 11821657,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8898166,
     "sourceId": 13959299,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28727.642287,
   "end_time": "2025-12-02T22:02:01.104816",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-02T14:03:13.462529",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
